<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="pytorch学习-pytorch快速入门, Hexo">
    <meta name="description" content="[toc]
本教程参考pytorch60分钟快速入门实现pytorch介绍Pytorch 是一个基于 Python 的科学计算库，它面向以下两种人群：

希望将其代替 Numpy 来利用 GPUs 的威力；
一个可以提供更加灵活和快速的深度">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>pytorch学习-pytorch快速入门 | Hexo</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 6.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Hexo</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Hexo</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/22.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">pytorch学习-pytorch快速入门</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/pytorch%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">pytorch学习</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2023-09-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2023-09-02
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    124 分
                </div>
                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <p>[toc]</p>
<h1 id="本教程参考pytorch60分钟快速入门实现"><a href="#本教程参考pytorch60分钟快速入门实现" class="headerlink" title="本教程参考pytorch60分钟快速入门实现"></a>本教程参考pytorch60分钟快速入门实现</h1><h2 id="pytorch介绍"><a href="#pytorch介绍" class="headerlink" title="pytorch介绍"></a>pytorch介绍</h2><p>Pytorch 是一个基于 Python 的科学计算库，它面向以下两种人群：</p>
<ul>
<li>希望将其代替 Numpy 来利用 GPUs 的威力；</li>
<li>一个可以提供更加灵活和快速的深度学习研究平台。</li>
</ul>
<h2 id="0-安装-Windows-补充，本质是创建一个pytorch虚拟环境"><a href="#0-安装-Windows-补充，本质是创建一个pytorch虚拟环境" class="headerlink" title="0 安装(Windows)(补充，本质是创建一个pytorch虚拟环境)"></a>0 安装(Windows)(补充，本质是创建一个pytorch虚拟环境)</h2><p>本安装教程采用<code>Conda</code>进行安装。</p>
<p>打开网站：<a target="_blank" rel="noopener" href="https://www.anaconda.com/">Anaconda | The World’s Most Popular Data Science Platform</a>2023年6月7号界面如下：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230607223630294.png" alt="image-20230607223630294"></p>
<p>选择<strong>Download</strong>进行下载，安装在安装路径界面要记住安装路径，之后全部默认即可。我的安装路径为：<code>C:\Users\Administrator\anaconda3</code>，这里也推荐默认即可。</p>
<p>安装后打开Anaconda Prompt软件：</p>
<p><img src="/./../pictures/pytorch%E5%AD%A6%E4%B9%A0-pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230607224118970.png" alt="image-20230607224118970"></p>
<p>输入：<code>conda creat -n pytorch python=3.6</code>这里3.6表示python版本。</p>
<p>然后就表示成功创建了<code>pytorch</code>环境。我们继续输入：<code>conda activate pytorch</code>激活<code>pytorch</code>环境。</p>
<p>我们继续安装<code>pytorch</code></p>
<p>点开网站：<a target="_blank" rel="noopener" href="https://pytorch.org/">PyTorch</a></p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230901205103352.png" alt="image-20230901205103352"></p>
<p>按照自己的需求选择，这里强调一点推荐使用<code>pip</code>代码进行安装，复制下面安装命令进入上一个页面的终端进行粘贴</p>
<p>完成安装。</p>
<h2 id="1-张量"><a href="#1-张量" class="headerlink" title="1 张量"></a>1 张量</h2><p>pytorch的一大作用是它可以替代Numpy库，pytorch的运算可以放在GPU进行计算。首先介绍pytorch的Tensors，Tensors也称为张量，它相当于Numpy的多维数组(ndarrays)。</p>
<p>使用torch，必须先导入相应的torch库。直接导入即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<p><strong>声明与定义</strong></p>
<p>对于Tensor的声明和定义，方法有如下几种：</p>
<ul>
<li>**torch.empty()**：声明一个空的矩阵。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个5*5空的矩阵</span></span><br><span class="line">x = torch.empty((<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.0000e+00</span>, <span class="number">0.0000e+00</span>, <span class="number">0.0000e+00</span>, <span class="number">0.0000e+00</span>, <span class="number">0.0000e+00</span>],</span><br><span class="line">        [<span class="number">5.9682e-02</span>, <span class="number">7.7052e+31</span>, <span class="number">7.2148e+22</span>, <span class="number">1.5766e-19</span>, <span class="number">1.0256e-08</span>],</span><br><span class="line">        [<span class="number">2.6252e-06</span>, <span class="number">3.1471e+12</span>, <span class="number">6.9366e-07</span>, <span class="number">1.7283e-04</span>, <span class="number">1.1040e-05</span>],</span><br><span class="line">        [<span class="number">1.2751e+16</span>, <span class="number">2.1707e-18</span>, <span class="number">7.0952e+22</span>, <span class="number">1.7748e+28</span>, <span class="number">1.8176e+31</span>],</span><br><span class="line">        [<span class="number">7.2708e+31</span>, <span class="number">5.0778e+31</span>, <span class="number">3.2608e-12</span>, <span class="number">1.7728e+28</span>, <span class="number">7.0367e+22</span>]])</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<p>实际中可以看到矩阵的输出值不是零，这里个人理解为：如果生成全零的话矩阵不安全。</p>
<ul>
<li>**torch.rand()**：随机初始化一个矩阵</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个5*5随机值的矩阵</span></span><br><span class="line">x = torch.rand(size=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.3108</span>, <span class="number">0.7507</span>, <span class="number">0.4726</span>, <span class="number">0.5343</span>, <span class="number">0.6130</span>],</span><br><span class="line">        [<span class="number">0.4937</span>, <span class="number">0.8821</span>, <span class="number">0.5618</span>, <span class="number">0.7130</span>, <span class="number">0.3910</span>],</span><br><span class="line">        [<span class="number">0.2435</span>, <span class="number">0.6003</span>, <span class="number">0.5142</span>, <span class="number">0.7005</span>, <span class="number">0.7096</span>],</span><br><span class="line">        [<span class="number">0.7607</span>, <span class="number">0.7926</span>, <span class="number">0.3843</span>, <span class="number">0.4276</span>, <span class="number">0.2849</span>],</span><br><span class="line">        [<span class="number">0.9941</span>, <span class="number">0.9155</span>, <span class="number">0.6728</span>, <span class="number">0.4572</span>, <span class="number">0.8649</span>]])</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<p>上述<code>torch.rand()</code>为从区间<code>[0, 1)</code>的均匀分布中抽取的一组随机数。还有类似的函数如下：</p>
<table>
<thead>
<tr>
<th><code>torch.randn()</code></th>
<th>从标准正态分布，即均值为0，方差为1的高斯分布中抽取一组随机数。</th>
</tr>
</thead>
<tbody><tr>
<td><strong><code>torch.randint()</code></strong></td>
<td><strong>随机生成a到b-1数字，其中a为自己设置的下界，b为上界</strong></td>
</tr>
</tbody></table>
<ul>
<li>**<code>torch.zeros()</code>**：创建一个数值都为0的矩阵</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个5*5的零矩阵</span></span><br><span class="line">x = torch.zeros(size=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<p>类似的也可以通过**<code>torch.ones()</code>**创建数值全部为1的矩阵。</p>
<ul>
<li>**<code>torch.tensor()</code>**：直接实现tensor数值的创建</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接创建tensor类型</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.<span class="built_in">type</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">&lt;built-<span class="keyword">in</span> method <span class="built_in">type</span> of Tensor <span class="built_in">object</span> at <span class="number">0x0000029A317AB6B0</span>&gt;</span><br></pre></td></tr></table></figure>

<p>除了上述的方法，也可以通过已经有的Tensor变量去创建新的Tensor变量，这样可以保留已有的Tensor变量的一些属性，如变量的尺寸大小，数值属性等。实现方法如下：</p>
<ul>
<li>**<code>tensor.new_ones()</code>*<em>：new_</em>()方法需要输入尺寸的大小：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tensor.new_ones</span></span><br><span class="line">x = torch.rand(size=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">y = x.new_ones(size=(<span class="number">5</span>, <span class="number">3</span>), dtype=torch.float32) <span class="comment">#形状自己定义</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<ul>
<li>**<code>torch.randn_like(old_tensor)</code>**：该方法保留尺寸大小</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch.randn_like</span></span><br><span class="line">z = torch.randn_like(y, dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="built_in">print</span>(z.shape)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">0.3874</span>, -<span class="number">1.3094</span>, -<span class="number">0.6167</span>],</span><br><span class="line">        [-<span class="number">0.0815</span>, -<span class="number">0.0609</span>, -<span class="number">0.1158</span>],</span><br><span class="line">        [-<span class="number">0.0993</span>, -<span class="number">0.3456</span>,  <span class="number">1.2239</span>],</span><br><span class="line">        [-<span class="number">0.4754</span>,  <span class="number">0.0983</span>,  <span class="number">0.3632</span>],</span><br><span class="line">        [ <span class="number">0.0545</span>, -<span class="number">1.9061</span>,  <span class="number">0.5378</span>]])</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>可以看出<code>z</code>和<code>y</code>的尺寸大小是一致的。</p>
<p>最后，如果要获取tensors的尺寸大小可以采取<code>tensor.size()</code>方法来获取：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取tensor的尺寸大小</span></span><br><span class="line"><span class="built_in">print</span>(z.size())</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">5</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>该结果是元组类型，它可以支持所有的元组操作。</p>
<p>当然，使用<code>tensor.shape</code>得到的结果一样。</p>
<p><strong>操作(Operations)</strong></p>
<p>操作实际上包含许多语法，这里作为快速入门，仅仅用加法实现，实际中还有转置、索引、切片、数学计算、线性代数、随机数等，可以参考：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html">torch — PyTorch 2.0 documentation</a></p>
<p>对于加法操作，实现方法如下：</p>
<ul>
<li>**<code>+</code>**运算符</li>
<li><strong><code>torch.add(tensor1, tensor2, [out=tensor3])</code></strong></li>
<li><strong><code>tensor1.add_(tensor2)</code></strong></li>
</ul>
<p>示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加法</span></span><br><span class="line">x = torch.rand(size=(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">y = torch.ones(size=(<span class="number">1</span>, <span class="number">5</span>), dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(x, <span class="string">&#x27;\n&#x27;</span>, y)</span><br><span class="line"><span class="comment"># 方法一</span></span><br><span class="line">add1 = x + y</span><br><span class="line"><span class="comment"># 方法二</span></span><br><span class="line">add2_1 = torch.add(x, y)</span><br><span class="line">add2_2 = torch.empty(size=(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">torch.add(x, y, out=add2_2)</span><br><span class="line"><span class="comment"># 方法三</span></span><br><span class="line">add3 = x.add_(y)</span><br><span class="line"><span class="built_in">print</span>(add1)</span><br><span class="line"><span class="built_in">print</span>(add2_1)</span><br><span class="line"><span class="built_in">print</span>(add2_2)</span><br><span class="line"><span class="built_in">print</span>(add3)</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.5714</span>, <span class="number">0.5090</span>, <span class="number">0.4586</span>, <span class="number">0.7658</span>, <span class="number">0.7157</span>]]) </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">tensor([[<span class="number">1.5714</span>, <span class="number">1.5090</span>, <span class="number">1.4586</span>, <span class="number">1.7658</span>, <span class="number">1.7157</span>]])</span><br><span class="line">tensor([[<span class="number">1.5714</span>, <span class="number">1.5090</span>, <span class="number">1.4586</span>, <span class="number">1.7658</span>, <span class="number">1.7157</span>]])</span><br><span class="line">tensor([[<span class="number">1.5714</span>, <span class="number">1.5090</span>, <span class="number">1.4586</span>, <span class="number">1.7658</span>, <span class="number">1.7157</span>]])</span><br><span class="line">tensor([[<span class="number">1.5714</span>, <span class="number">1.5090</span>, <span class="number">1.4586</span>, <span class="number">1.7658</span>, <span class="number">1.7157</span>]])</span><br></pre></td></tr></table></figure>

<p><em><strong>注意</strong></em>：可以改变tensor变量的操作都带有一个后缀<code>_</code>，例如<code>x.copy_(y)</code>和<code>x.t_()</code>都可以改变变量<code>x</code>的值。</p>
<p>除了加法操作，对于Tensor的访问，和Numpy对数据类似，可以使得索引来访问某一维的数据，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据访问</span></span><br><span class="line">x = torch.rand(size=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># 访问x的第一列</span></span><br><span class="line"><span class="built_in">print</span>(x[:, <span class="number">0</span>])</span><br><span class="line"><span class="comment"># 访问x的第一行</span></span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>, :])</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.6596</span>, <span class="number">0.9104</span>, <span class="number">0.0697</span>, <span class="number">0.1354</span>, <span class="number">0.9784</span>],</span><br><span class="line">        [<span class="number">0.8394</span>, <span class="number">0.9843</span>, <span class="number">0.1589</span>, <span class="number">0.0508</span>, <span class="number">0.2282</span>],</span><br><span class="line">        [<span class="number">0.7638</span>, <span class="number">0.5745</span>, <span class="number">0.6807</span>, <span class="number">0.4535</span>, <span class="number">0.3799</span>],</span><br><span class="line">        [<span class="number">0.0837</span>, <span class="number">0.6831</span>, <span class="number">0.0075</span>, <span class="number">0.1065</span>, <span class="number">0.5147</span>],</span><br><span class="line">        [<span class="number">0.6209</span>, <span class="number">0.4875</span>, <span class="number">0.1491</span>, <span class="number">0.2534</span>, <span class="number">0.7124</span>]])</span><br><span class="line">tensor([<span class="number">0.6596</span>, <span class="number">0.8394</span>, <span class="number">0.7638</span>, <span class="number">0.0837</span>, <span class="number">0.6209</span>])</span><br><span class="line">tensor([<span class="number">0.6596</span>, <span class="number">0.9104</span>, <span class="number">0.0697</span>, <span class="number">0.1354</span>, <span class="number">0.9784</span>])</span><br></pre></td></tr></table></figure>

<p>如果需要对Tensor的尺度进行修改，我们可以利用<code>torch.view()</code>函数，示例如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改尺度</span></span><br><span class="line">x = torch.rand((<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(x, x.shape)</span><br><span class="line"><span class="comment"># 先全部展品平</span></span><br><span class="line">y0 = x.view(<span class="number">16</span>)</span><br><span class="line"><span class="built_in">print</span>(y0, y0.shape)</span><br><span class="line"><span class="comment"># 将其化成2 * 8矩阵</span></span><br><span class="line">y1 = x.view(<span class="number">2</span>, <span class="number">8</span>)</span><br><span class="line"><span class="built_in">print</span>(y1, y1.shape)</span><br><span class="line">y2 = x.view(-<span class="number">1</span>, <span class="number">8</span>)</span><br><span class="line"><span class="built_in">print</span>(y2, y2.shape)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.3901</span>, <span class="number">0.3256</span>, <span class="number">0.1368</span>, <span class="number">0.7826</span>],</span><br><span class="line">        [<span class="number">0.2582</span>, <span class="number">0.2975</span>, <span class="number">0.3874</span>, <span class="number">0.8776</span>],</span><br><span class="line">        [<span class="number">0.7247</span>, <span class="number">0.9416</span>, <span class="number">0.8406</span>, <span class="number">0.6932</span>],</span><br><span class="line">        [<span class="number">0.2741</span>, <span class="number">0.0752</span>, <span class="number">0.5778</span>, <span class="number">0.0013</span>]]) torch.Size([<span class="number">4</span>, <span class="number">4</span>])</span><br><span class="line">tensor([<span class="number">0.3901</span>, <span class="number">0.3256</span>, <span class="number">0.1368</span>, <span class="number">0.7826</span>, <span class="number">0.2582</span>, <span class="number">0.2975</span>, <span class="number">0.3874</span>, <span class="number">0.8776</span>, <span class="number">0.7247</span>,</span><br><span class="line">        <span class="number">0.9416</span>, <span class="number">0.8406</span>, <span class="number">0.6932</span>, <span class="number">0.2741</span>, <span class="number">0.0752</span>, <span class="number">0.5778</span>, <span class="number">0.0013</span>]) torch.Size([<span class="number">16</span>])</span><br><span class="line">tensor([[<span class="number">0.3901</span>, <span class="number">0.3256</span>, <span class="number">0.1368</span>, <span class="number">0.7826</span>, <span class="number">0.2582</span>, <span class="number">0.2975</span>, <span class="number">0.3874</span>, <span class="number">0.8776</span>],</span><br><span class="line">        [<span class="number">0.7247</span>, <span class="number">0.9416</span>, <span class="number">0.8406</span>, <span class="number">0.6932</span>, <span class="number">0.2741</span>, <span class="number">0.0752</span>, <span class="number">0.5778</span>, <span class="number">0.0013</span>]]) torch.Size([<span class="number">2</span>, <span class="number">8</span>])</span><br><span class="line">tensor([[<span class="number">0.3901</span>, <span class="number">0.3256</span>, <span class="number">0.1368</span>, <span class="number">0.7826</span>, <span class="number">0.2582</span>, <span class="number">0.2975</span>, <span class="number">0.3874</span>, <span class="number">0.8776</span>],</span><br><span class="line">        [<span class="number">0.7247</span>, <span class="number">0.9416</span>, <span class="number">0.8406</span>, <span class="number">0.6932</span>, <span class="number">0.2741</span>, <span class="number">0.0752</span>, <span class="number">0.5778</span>, <span class="number">0.0013</span>]]) torch.Size([<span class="number">2</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure>

<p>如果tensor只有一个元素，我么可以采用<code>tensor.item()</code>来获取类似Python中整数类型的数值，也就是将其去掉<code>tensor</code>类型化为python普通的类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.item())</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">0.9035</span>])</span><br><span class="line"><span class="number">0.9034661650657654</span></span><br></pre></td></tr></table></figure>

<p>更多的运算可以参考官方文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html">torch — PyTorch 2.0 documentation</a></p>
<h2 id="2-和Numpy数组的转换"><a href="#2-和Numpy数组的转换" class="headerlink" title="2 和Numpy数组的转换"></a>2 和Numpy数组的转换</h2><p>Tensor和Numpy的数组可以相互进行转换，并且在转换后共享在CPU下的内存空间，即改变其中的一个数值，另一个变量也将会发生变化。但是Numpy无法通过GPU实现计算，Tensor可以很方便的部署到GPU进行运算。</p>
<p><strong>Tensor转换为Numpy数组</strong></p>
<p>想要实现Tensor数据类型转换为numpy数据类型可以直接使用<code>tensor.numpy()</code>方法进行转换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tensor转numpy</span></span><br><span class="line">x = torch.rand(<span class="number">5</span>)</span><br><span class="line">y = x.numpy()</span><br><span class="line">x.<span class="built_in">type</span>(), <span class="built_in">type</span>(y)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&#x27;torch.FloatTensor&#x27;</span>, numpy.ndarray)</span><br></pre></td></tr></table></figure>

<p>此外，两者是共享同一个内存空间的，此时修改<code>x</code>或者<code>y</code>时，另一方都会相应的发生变化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x, y共享空间</span></span><br><span class="line"><span class="built_in">print</span>(x, y)</span><br><span class="line">x.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x, y)</span><br><span class="line">y[<span class="number">0</span>] = <span class="number">9999</span></span><br><span class="line"><span class="built_in">print</span>(x, y)</span><br></pre></td></tr></table></figure>

<p>输出的结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1.9986</span>, <span class="number">1.3738</span>, <span class="number">1.4414</span>, <span class="number">1.3637</span>, <span class="number">1.9343</span>]) [<span class="number">1.998558</span>  <span class="number">1.3738213</span> <span class="number">1.4413819</span> <span class="number">1.363684</span>  <span class="number">1.9343324</span>]</span><br><span class="line">tensor([<span class="number">2.9986</span>, <span class="number">2.3738</span>, <span class="number">2.4414</span>, <span class="number">2.3637</span>, <span class="number">2.9343</span>]) [<span class="number">2.998558</span>  <span class="number">2.3738213</span> <span class="number">2.441382</span>  <span class="number">2.3636842</span> <span class="number">2.9343324</span>]</span><br><span class="line">tensor([<span class="number">9.9990e+03</span>, <span class="number">2.3738e+00</span>, <span class="number">2.4414e+00</span>, <span class="number">2.3637e+00</span>, <span class="number">2.9343e+00</span>]) [<span class="number">9.9990000e+03</span> <span class="number">2.3738213e+00</span> <span class="number">2.4413819e+00</span> <span class="number">2.3636842e+00</span> <span class="number">2.9343324e+00</span>]</span><br></pre></td></tr></table></figure>

<p><strong>Numpy数组转换为Tensor</strong></p>
<p>要将Numpy数据转换为Tensor数据可以利用方法：<code>torch.from_numpy(numpy_array)</code>来实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.random.randn(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(x))</span><br><span class="line">y = torch.from_numpy(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(y))</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;torch.Tensor&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>

<p>在CPU上，除了<code>CharTensor</code>外的所以<code>Tensor</code>类型变量都支持和<code>Numpy</code>数组的相互转换。</p>
<h2 id="3-CUDA张量"><a href="#3-CUDA张量" class="headerlink" title="3 CUDA张量"></a>3 CUDA张量</h2><p><code>Tensor</code>可以通过方法<code>.to</code>转到不同的设备(CPU、GPU)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在cpu和gpu进行转换</span></span><br><span class="line"><span class="comment"># 定义获取设备函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">try_gpu</span>():</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x.device)</span><br><span class="line">x = x.to(try_gpu())</span><br><span class="line"><span class="built_in">print</span>(x.device)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cpu</span><br><span class="line">cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure>

<h2 id="4-autograd库"><a href="#4-autograd库" class="headerlink" title="4 autograd库"></a>4 autograd库</h2><p>autograd是一个深度学习的库，它的功能是对Tensor上所有的运算操作实现了自动微分功能，也就是可以实现自动计算梯度。它属于<code>define-by-run</code>框架。反向传播的定义是根据代码的运行方式，每次迭代都是不同的。</p>
<p><strong>张量</strong></p>
<p><code>torch.Tensor</code>是Pythoch最主要的库，有一个该变量，当设置它的属性为<code>.requires_grad=True</code>,那么就会开始追踪这个变量上的所有操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个tensor类型变量，并且让require_grad</span></span><br><span class="line">x = torch.ones(<span class="number">5</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>执行操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># y=x+2</span></span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>y是一个操作结果，所以他带有属性<code>grad_fn</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(y.grad_fn)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;AddBackward0 <span class="built_in">object</span> at <span class="number">0x000002102933CEE0</span>&gt;</span><br></pre></td></tr></table></figure>

<p>继续对y变量进行操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z = <span class="number">3</span> * y * y</span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>],</span><br><span class="line">        [<span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>],</span><br><span class="line">        [<span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>],</span><br><span class="line">        [<span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>],</span><br><span class="line">        [<span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>, <span class="number">27.</span>]], grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor(<span class="number">27.</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>实际上，一个<code>tensor</code>变量的默认<code>requires_grad</code>是<code>False</code>，可以在定义的时候设置该属性是<code>True</code>，也可以在定义变量后，调用<code>.requires_grad_(True)</code>设置为<code>True</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">x = ((x * <span class="number">3</span>) / (x - <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(x.requires_grad)</span><br><span class="line">x.requires_grad_(<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x.requires_grad)</span><br><span class="line">y = (x * x).<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(y.grad_fn)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line">&lt;SumBackward0 <span class="built_in">object</span> at <span class="number">0x00000210294ACEB0</span>&gt;</span><br></pre></td></tr></table></figure>

<p><strong>梯度</strong></p>
<p>可以调用<code>.backward()</code>可以计算该变量所有的梯度，得到的结果通过属性<code>.grad</code>获得。在张量的内容中，我们定义了<code>x</code>、<code>y</code>、<code>z</code>以及<code>out</code>。我们通过数学与代码结合起来进行解释梯度。</p>
<p>首先定义一个<code>x</code>，它是一个全1的矩阵，变量的梯度属性打开。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">5</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>设置<code>y</code>为<code>x+2</code>如下，如果此时对y进行求导，理论上得到是全1的值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">5</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>

<p>对于结果是梯度，求导需要加上<code>sum()</code>实现。</p>
<p>再令<code>z = 3*y*y</code>可以知道实际上<code>z=3*(x + 2)*(x + 2)</code>，对<code>z</code>关于<code>x</code>进行求导理论上得到的结果是：<code>6*(x + 2)</code>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z = <span class="number">3</span> * y * y</span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>],</span><br><span class="line">        [<span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>],</span><br><span class="line">        [<span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>],</span><br><span class="line">        [<span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>],</span><br><span class="line">        [<span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>, <span class="number">19.</span>]])</span><br></pre></td></tr></table></figure>

<p>这与我们理论上的结果有偏差，实际上是在求<code>z</code>关于<code>x</code>的导数时应该将<code>y</code>对<code>x</code>的倒数进行清零操作，不然结果会出现叠加，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x.grad = torch.zeros(x.shape)</span><br><span class="line">z = <span class="number">3</span> * y * y</span><br><span class="line">z.<span class="built_in">sum</span>().backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>],</span><br><span class="line">        [<span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>],</span><br><span class="line">        [<span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>],</span><br><span class="line">        [<span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>],</span><br><span class="line">        [<span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>, <span class="number">18.</span>]])</span><br></pre></td></tr></table></figure>

<p>定义<code>out=z.mean()</code>再进行求导理论上就是：<code>(1/25)*6*(x + 2)</code>这里的<code>x=1</code>也就是结果是<code>0.72</code>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x.grad = torch.zeros(x.shape)</span><br><span class="line">out = z.mean()</span><br><span class="line">out.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment"># 上一步y的求导一定要将retain_graph=True加上，否则会报错。</span></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>],</span><br><span class="line">        [<span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>],</span><br><span class="line">        [<span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>],</span><br><span class="line">        [<span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>],</span><br><span class="line">        [<span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>, <span class="number">0.7200</span>]])</span><br></pre></td></tr></table></figure>

<p>上述基本上实现一个简单的解释，通过数学进一步解释，如果有一个向量值函数：<br>$$<br>\overrightarrow{y}&#x3D;f\left( \overrightarrow{x} \right)<br>$$<br>俺么对应的梯度是一个雅克比矩阵(Jacobian matrix)：<br>$$<br>J&#x3D;\left[ \begin{matrix}<br>    \frac{\partial y_1}{\partial x_1}&amp;		\cdots&amp;		\frac{\partial y_1}{\partial x_n}\<br>    \vdots&amp;		\ddots&amp;		\vdots\<br>    \frac{\partial y_n}{\partial x_1}&amp;		\cdots&amp;		\frac{\partial y_n}{\partial x_n}\<br>\end{matrix} \right]<br>$$<br>一般来说，<code>torch.autograd</code>就是用于计算雅克比向量乘积的工具。示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ -<span class="number">676.7817</span>, -<span class="number">1410.8573</span>,   <span class="number">143.9314</span>], grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>这里得到的变量<code>y</code>不再是一个标量，<code>torch.autograd</code>不能直接计算完整的雅克比行列式，但是可以将向量传给<code>backward()</code>方法作为参数得到雅克比向量的乘积，例子如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1.0240e+02</span>, <span class="number">1.0240e+03</span>, <span class="number">1.0240e-01</span>])</span><br></pre></td></tr></table></figure>

<p>我的理解<code>v</code>是指定<code>x</code>的值。</p>
<p>最后加上<code>with torch.no_grad()</code>就可以停止追踪变量历史进行自动梯度计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.requires_grad)</span><br><span class="line"><span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>

<p><strong>其它知识</strong></p>
<p>调用<code>.detach()</code>方法分离出计算的历史，可以停止一个tensor变量继续追踪其历史信息，同时也会防止未来的计算会被追踪。</p>
<p>如果希望防止跟踪历史，可以将代码块放在<code>with torch.no_grad():</code>内，这样用于评估一个模型非常有用。</p>
<p>对于<code>autograd</code>的实现，有一个类非常重要：<code>function</code>。</p>
<p><code>Tensor</code>和<code>function</code>两个类都是关联了一个肺循环的图，可以编码一个完整的计算记录。每个<code>tensor</code>变量都带有属性<code>.grad_fn</code>，该属性引用了创建了这个变量的<code>Function</code>(用户创建的<code>Tensor</code>一般有<code>grad_fn=None</code>)</p>
<p>如果要进行求导计算，可以调用一个<code>Tensor</code>变量的方法<code>.backward()</code>。如果该变量是一个标量不需要传递参数，如果是一个向量，需要自己指定<code>gradient</code>参数。</p>
<p><strong>处理梯度代码报错</strong></p>
<p>在我们求梯度时会出现一类代码错误，有提示词：<code>RuntimeError: Trying to backward through the graph a second time</code>首先我们应该知道的概念：在pytorch计算中，实际上只有两种元素：tensor和function，function就是加减乘除、开方、三角函数等可以求导的运算。Tensor可以分为两类：叶子节点(leaf node)和非叶子节点。使用<code>.backward()</code>计算tensor的梯度时，实际上不计算所有的<code>tensor</code>的梯度，只计算满足下面条件的梯度：1.类型是叶子节点；2.requires_grad&#x3D;True;3.依赖的tensor的所有tensor都是requires_grad&#x3D;True。</p>
<p>看看下面代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">z = y ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">m = y.mean()</span><br><span class="line">n = z.mean()</span><br><span class="line"></span><br><span class="line">m.backward()</span><br><span class="line">n.backward()</span><br></pre></td></tr></table></figure>

<p>执行的时候会出现上述报错</p>
<p>根据这段代码画出计算图：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230608124327049.png" alt="image-20230608124327049"></p>
<p>当我们进行<code>m.backward()</code>之后，下面红框计算图被破坏：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230608124422607.png" alt="image-20230608124422607"></p>
<p>很明显y不是叶子节点，不属于<code>m.backward()</code>里面要计算的tensor，也就是可以理解计算<code>m.backward()</code>后只会保留x的梯度，y的梯度被释放。所以后面求解会出错。如果想保留y的梯度可以通过设置<code>retain_graph</code>属性实现：<code>m.backward(retain_graph=True)</code></p>
<p>再看一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">z = x ** <span class="number">3</span></span><br><span class="line"></span><br><span class="line">m = y.mean()</span><br><span class="line">n = z.mean()</span><br><span class="line"></span><br><span class="line">m.backward()</span><br><span class="line">n.backward()</span><br></pre></td></tr></table></figure>

<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230608124837880.png" alt="image-20230608124837880"></p>
<p>这段代码可以正常执行，虽然他们计算图共享了<code>x</code>，<code>x</code>是叶子节点且<code>requires_grad=True</code>，经过<code>backward</code>后它的梯度会保留。这个代码最后输出的结果是<code>n.backward()</code>之后的结果，前面的<code>x</code>的梯度将会被替代。</p>
<p>最后再来介绍以下<code>detach</code>的作用。<code>detach</code>可以把一个非叶子节点变成叶子节点且：<code>requires_grad=False</code>看下图：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230608125310613.png" alt="image-20230608125310613"></p>
<p>如果把z变成<code>z.detach()</code>，流向如下：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230608125406160.png" alt="image-20230608125406160"></p>
<p><code>detach</code>可以起到截流作用，相当于把<code>z</code>看成一个常数。</p>
<h2 id="5-神经网络"><a href="#5-神经网络" class="headerlink" title="5 神经网络"></a>5 神经网络</h2><p>在pytorch中<code>torch.nn</code>专门实现神经网络。其中<code>nn.Module</code>包含了网络层的搭建，以及一个方法<code>forward(input)</code>，并且返回输出<code>output</code>。</p>
<p>下图是一个典型的LeNet网络，用于对字符进行分类。</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230608133105245.png" alt="image-20230608133105245"></p>
<p>对于神经网络来说，其标准的流程如下：</p>
<ul>
<li>定义一个多层的神经网络</li>
<li>对数据集的预处理并准备作为网络的输入</li>
<li>将数据输入到网络</li>
<li>计算网络损失</li>
<li>反向传播、计算梯度</li>
<li>更新网络梯度，一个简单的更新规则是<code>weight = weight - learning_rate * gradient</code></li>
</ul>
<p><strong>定义网络</strong></p>
<p>首先定义一个神经网络如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 输入单通道，输出6通道</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 卷积层输入6输出16</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 除了batch维度外的所有维度</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]</span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>

<p>输出的结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Net(</span><br><span class="line">  (conv1): Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (conv2): Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (fc1): Linear(in_features=<span class="number">400</span>, out_features=<span class="number">120</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc2): Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc3): Linear(in_features=<span class="number">84</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>这里必须实现<code>forward</code>函数，而<code>backward</code>函数在采用<code>autograd</code>时就自动定义好了，在<code>forward</code>方法可以采用任何张量操作。</p>
<p><code>net.parameters()</code>可以返回网络的训练参数，使用例子如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">paras = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(paras))</span><br><span class="line"><span class="built_in">print</span>(paras[<span class="number">0</span>].size())</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span></span><br><span class="line">torch.Size([<span class="number">6</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<p>然后简单的测试该网络，随机生成32*32的输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net_input = torch.randn(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">out = net(net_input)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-<span class="number">0.0824</span>,  <span class="number">0.0089</span>, -<span class="number">0.0071</span>,  <span class="number">0.0677</span>,  <span class="number">0.0313</span>, -<span class="number">0.0424</span>, -<span class="number">0.0922</span>,  <span class="number">0.1634</span>,</span><br><span class="line">         -<span class="number">0.1603</span>,  <span class="number">0.0420</span>]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>接着反向传播需要先清空梯度缓存，并反向传播随机梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn((<span class="number">1</span>, <span class="number">10</span>)))</span><br></pre></td></tr></table></figure>

<p>注意：<code>torch.nn</code>只支持小批量数据，输入不能是单个样本，比如<code>nn.Conv2d</code>接受的输入是一个4维张量：<code>nSamples * nChannels * Height * Width</code>。所以，输入是单个样本，需要采用<code>input.unsqueeze(0)</code>来扩充一个假的<code>batch</code>维度，即从3维到4维。</p>
<p><strong>损失函数</strong></p>
<p>损失函数的输入是<code>(output, target)</code>，即网络输出和真实标签的数据，然后返回一个数值表示网络输出和真实标签的差距。</p>
<p>pytorch自带许多损失函数，这里采用<code>nn.MSELoss</code>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">output = net(net_input)</span><br><span class="line"><span class="comment"># 定义伪标签</span></span><br><span class="line"><span class="comment"># labels = torch.randn(size=output.shape)</span></span><br><span class="line">labels = torch.randn(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 调整大小使得和输出一样的大小</span></span><br><span class="line">labels = labels.view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, labels)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">1.3829</span>, grad_fn=&lt;MseLossBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>整个计算流程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure>

<p>如果只调用<code>loss.backward()</code>那么整个图都是可微分的，也就是包括<code>loss</code>。只要张量的属性<code>requires_grad=True</code>那么全部的张量都是可以微分的。梯度<code>.grad</code>也会一起跟着累计。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MSELoss</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn)</span><br><span class="line"><span class="comment"># Linear layer</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line"><span class="comment"># ReLU</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p>输出结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;MseLossBackward0 <span class="built_in">object</span> at <span class="number">0x000002100F915960</span>&gt;</span><br><span class="line">&lt;AddmmBackward0 <span class="built_in">object</span> at <span class="number">0x000002100F915540</span>&gt;</span><br><span class="line">&lt;AccumulateGrad <span class="built_in">object</span> at <span class="number">0x000002100F915810</span>&gt;</span><br></pre></td></tr></table></figure>

<p><strong>反向传播</strong></p>
<p>反向传播只需要调用<code>loss.backward()</code>即可。使用前要清除梯度缓存。</p>
<p>以<code>conv1</code>层的偏置参数为例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad before backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad after backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conv1.bias.grad before backward</span><br><span class="line"><span class="literal">None</span></span><br><span class="line">conv1.bias.grad after backward</span><br><span class="line">tensor([ <span class="number">0.0112</span>, -<span class="number">0.0163</span>,  <span class="number">0.0141</span>, -<span class="number">0.0029</span>, -<span class="number">0.0208</span>, -<span class="number">0.0029</span>])</span><br></pre></td></tr></table></figure>

<p><strong>更新权重</strong></p>
<p>采用随机梯度下降方法的最简单的更新权重规则如下：</p>
<p><code>weight = weight - learning_rate * gradient</code></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(learning_rate * f.grad.data)</span><br></pre></td></tr></table></figure>

<p>但是这只是最简单的规则，深度学习有许多优化算法，不仅仅是<code>SGD</code>，还有<code>Nesterov-SGD</code>、<code>Adam</code>、<code>RMSProp</code>等等，为了采用这些不同的方法，这里采用<code>torch.optim</code>库，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">optimizer.zero_grad() <span class="comment"># 清空梯度缓存</span></span><br><span class="line">output = net(net_input)</span><br><span class="line">loss = criterion(output, labels)</span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment"># 更新权重</span></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>

<p>注意：使用优化器也需要进行梯度的清除。</p>
<h2 id="6-训练分类器"><a href="#6-训练分类器" class="headerlink" title="6 训练分类器"></a>6 训练分类器</h2><p><strong>训练数据</strong></p>
<p>在训练分类，我们需要考虑数据问题，在处理图片、文本、语音或者视频数据的时候，一般都采用标准python库将其加载并转换成Numpy数组，然后返回Pythoch的张量。</p>
<ul>
<li>对于图像，可以采用<code>Pillow, OpenCV</code>库；</li>
<li>对于语言，有<code>scipy</code>和<code>librosa</code>；</li>
<li>对于文本，可以选择原生python或者Cython进行加载数据，或者用<code>NLTK</code>和<code>SpaCy</code>。</li>
</ul>
<p>PyTorch 对于计算机视觉，特别创建了一个 <code>torchvision</code> 的库，它包含一个数据加载器(data loader)，可以加载比较常见的数据集，比如 <code>Imagenet, CIFAR10, MNIST</code> 等等，然后还有一个用于图像的数据转换器(data transformers)，调用的库是 <code>torchvision.datasets</code> 和 <code>torch.utils.data.DataLoader</code> 。</p>
<p>本教程采用<code>CIFA10</code>数据集</p>
<p><strong>训练图片分类器</strong></p>
<ul>
<li><p>通过<code>torchvision</code>加载和归一化<code>CIFA10</code>训练集和测试集</p>
<ul>
<li><p>首先导入必须的包：</p>
</li>
<li><pre><code class="python">import torch
import torchvision
from torchvision import transforms
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- `torchvision`的数据集输出的图片都是`PILImage`，取值范围是[0, 1]，这里需要做一个变换，将其变为[-1 1]，如下：</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  transform = transforms.Compose([transforms.ToTensor(),</span><br><span class="line">                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])</span><br><span class="line">  trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True,</span><br><span class="line">                                          download=True, transform=transform)</span><br><span class="line">  trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,</span><br><span class="line">                                            shuffle=True, num_workers=2)</span><br><span class="line">  testset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=False,</span><br><span class="line">                                         download=True, transform=transform)</span><br><span class="line">  testloader = torch.utils.data.DataLoader(testset, batch_size=4,</span><br><span class="line">                                            shuffle=False, num_workers=2)</span><br><span class="line">  classes = (&#x27;plane&#x27;, &#x27;car&#x27;, &#x27;bird&#x27;, &#x27;cat&#x27;, &#x27;deer&#x27;,</span><br><span class="line">             &#x27;dog&#x27;, &#x27;frog&#x27;, &#x27;horse&#x27;, &#x27;ship&#x27;, &#x27;truck&#x27;)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>下载好数据后可视化训练图片：</p>
</li>
<li><pre><code class="python"># 训练可视化
import matplotlib.pyplot as plt
import numpy as np

# 展示图片的函数
def imshow(img):
    img = img / 2 + 0.5 # 非归一化
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# 随机获取训练集图片
dataiter = next(iter(trainloader))
images, labels = dataiter

# 展示图片
imshow(torchvision.utils.make_grid(images))
# 打印类别
print(&#39;&#39;.join(&#39;%5s&#39; % classes[labels[j]] for j in range(4)))
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 展示的图片如下：</span><br><span class="line"></span><br><span class="line">- ![image-20230608150739911](../pictures/pytorch快速入门/image-20230608150739911-1686208060114-1.png)</span><br><span class="line"></span><br><span class="line">- 对应的输出结果：</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  horse ship  car  car</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p>建立一个卷积神经网络</p>
<ul>
<li><p>直接采用上面的网络</p>
</li>
<li><pre><code class="python">import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 定义一个损失函数</span><br><span class="line"></span><br><span class="line">  - 这里采用`SGD`的优化方法</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    from torch import optim</span><br><span class="line">    </span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p>在训练集上训练网络</p>
<ul>
<li><p>训练网络需要迭代的<code>epoch</code>，然后输入数据，指定次数打印当前网络的信息，比如loss或者评价网络准确率的信息。</p>
</li>
<li><pre><code class="python">import time
start = time.time()
epochs = 2

# 开始训练
for epoch in range(epochs):
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # 获取数据
        features, labels = data
        # 清除缓存
        optimizer.zero_grad()

        outputs = net(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # 打印统计信息
        running_loss += loss.data
        if (i + 1) % 2000 == 0:
            print(f&quot;epoch: [&#123;epoch + 1&#125;, &#123;i + 1&#125;], loss: &#123;running_loss / 2000:.3f&#125;&quot;)
            running_loss = 0.0
print(f&quot;训练总共用时：&#123;time.time() - start&#125;&quot;)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 结果输出：</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  epoch: [1, 2000], loss: 2.211</span><br><span class="line">  epoch: [1, 4000], loss: 2.141</span><br><span class="line">  epoch: [1, 6000], loss: 2.093</span><br><span class="line">  epoch: [1, 8000], loss: 2.076</span><br><span class="line">  epoch: [1, 10000], loss: 2.084</span><br><span class="line">  epoch: [1, 12000], loss: 2.057</span><br><span class="line">  epoch: [2, 2000], loss: 2.056</span><br><span class="line">  epoch: [2, 4000], loss: 2.099</span><br><span class="line">  epoch: [2, 6000], loss: 2.082</span><br><span class="line">  epoch: [2, 8000], loss: 2.099</span><br><span class="line">  epoch: [2, 10000], loss: 2.110</span><br><span class="line">  epoch: [2, 12000], loss: 2.111</span><br><span class="line">  训练总共用时：77.78270435333252</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p>在测试集上测试网络性能</p>
<ul>
<li><p>训练好需要进行测试，分类一般使用准确率作为衡量标准。</p>
</li>
<li><pre><code class="python">import time
start = time.time()
epochs = 2

# 开始训练
for epoch in range(epochs):
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # 获取数据
        features, labels = data
        # 清除缓存
        optimizer.zero_grad()

        outputs = net(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # 打印统计信息
        running_loss += loss.data
        if (i + 1) % 2000 == 0:
            print(f&quot;epoch: [&#123;epoch + 1&#125;, &#123;i + 1&#125;], loss: &#123;running_loss / 2000:.3f&#125;&quot;)
            running_loss = 0.0

    # 进行训练观察模型的准确
    correct = 0.0
    total = 0.0
    with torch.no_grad():
        for data in testloader:
            features, labels = data
            outputs = net(features)
            _, predicts = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicts == labels).sum().item()
        print(f&quot;第&#123;epoch + 1&#125; 轮，模型的准确率为：&#123;100 * correct / total&#125; %&quot;)

print(f&quot;训练总共用时：&#123;time.time() - start&#125;&quot;)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 结果输出如下：</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  epoch: [1, 2000], loss: 2.096</span><br><span class="line">  epoch: [1, 4000], loss: 2.088</span><br><span class="line">  epoch: [1, 6000], loss: 2.065</span><br><span class="line">  epoch: [1, 8000], loss: 2.050</span><br><span class="line">  epoch: [1, 10000], loss: 2.179</span><br><span class="line">  epoch: [1, 12000], loss: 2.052</span><br><span class="line">  第1 轮，模型的准确率为：23.61 %</span><br><span class="line">  epoch: [2, 2000], loss: 2.028</span><br><span class="line">  epoch: [2, 4000], loss: 2.035</span><br><span class="line">  epoch: [2, 6000], loss: 2.122</span><br><span class="line">  epoch: [2, 8000], loss: 2.103</span><br><span class="line">  epoch: [2, 10000], loss: 2.064</span><br><span class="line">  epoch: [2, 12000], loss: 2.189</span><br><span class="line">  第2 轮，模型的准确率为：12.3 %</span><br><span class="line">  训练总共用时：91.92787885665894</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>可以看出，第一轮的模型训练更好一点。</p>
</li>
<li><p>改进加上图片显示代码：</p>
</li>
<li><pre><code class="python">import time
start = time.time()
epochs = 2

# 开始训练
for epoch in range(epochs):
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # 获取数据
        features, labels = data
        # 清除缓存
        optimizer.zero_grad()

        outputs = net(features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # 打印统计信息
        running_loss += loss.data
        if (i + 1) % 2000 == 0:
            print(f&quot;epoch: [&#123;epoch + 1&#125;, &#123;i + 1&#125;], loss: &#123;running_loss / 2000:.3f&#125;&quot;)
            running_loss = 0.0

    # 进行训练观察模型的准确
    correct = 0.0
    total = 0.0
    with torch.no_grad():
        for i, data in enumerate(testloader, 0):
            features, labels = data
            outputs = net(features)
            _, predicts = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicts == labels).sum().item()
            # 2000次输出一次
            if (i + 1) % 2000 == 0:
                imshow(torchvision.utils.make_grid(features))
                print(&#39;预测值：&#39;, &#39;&#39;.join(&#39;%5s&#39; % classes[predicts[j]] for j in range(4)))
        print(f&quot;第&#123;epoch + 1&#125; 轮，模型的准确率为：&#123;100 * correct / total&#125; %&quot;)
print(f&quot;训练总共用时：&#123;time.time() - start&#125;&quot;)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 实际中我们还可以进一步，测试每个类别的预测准确率：</span><br><span class="line"></span><br><span class="line">- 跟上述代码不同的是，类别计算正确率的部分为：`c = (predicted == labels).squeeze()`，这段代码会根据预测值和真实标签是否相等，输出1或者0来表示真与假。在计算当前类别正确预测数量时直接相加，预测正确加一，错误为0没有变化。</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  # 进行训练观察模型的准确</span><br><span class="line">      correct = [0.0 for _ in range(10)]</span><br><span class="line">      total = [0.0 for _ in range(10)]</span><br><span class="line">      with torch.no_grad():</span><br><span class="line">          for i, data in enumerate(testloader, 0):</span><br><span class="line">              features, labels = data</span><br><span class="line">              outputs = net(features)</span><br><span class="line">              _, predicts = torch.max(outputs.data, 1)</span><br><span class="line">              c = (predicts == labels).squeeze()</span><br><span class="line">              for j in range(4):</span><br><span class="line">                  label = labels[j]</span><br><span class="line">                  correct[label] += c[j].item()</span><br><span class="line">                  total[label] += 1</span><br><span class="line">          for k in range(10):</span><br><span class="line">              print(f&quot;对于&#123;classes[k]&#125;，模型的准确率为：&#123;100 * correct[k] / total[k]&#125; %&quot;)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="7-在GPU上训练"><a href="#7-在GPU上训练" class="headerlink" title="7 在GPU上训练"></a>7 在GPU上训练</h2><p>通过输入：<code>cuda.is_available()</code>来观察GPU是否可用，如果可用将会返回<code>True</code>，反之。</p>
<p>在gpu训练只需要将：<strong>网络、数据、损失可以放或者不放</strong>放在GPU即可。先将网络放如GPU，再从写一次优化器的化不影响损失数据，这样操作理论上损失会自动放到GPU。</p>
<h2 id="8-数据并行-多GPU"><a href="#8-数据并行-多GPU" class="headerlink" title="8 数据并行(多GPU)"></a>8 数据并行(多GPU)</h2><p>这部分将学如何使用多个GPU来跑代码。</p>
<p>首先，在GPU上训练模型的做法比较简单，定义一个<code>device</code>对象，然后用<code>.to</code>的方法将模型参数放在指定的GPU即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>

<p>接着就是将所有的张量变量放到GPU上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mytensor = my_tensor.to(device)</span><br></pre></td></tr></table></figure>

<p>Pytorch默认会采用一个GPU，因此需要使用多个GPU需要采用<code>DataParallel</code>，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = nn.DataParallel(model)</span><br></pre></td></tr></table></figure>

<p><strong>导入和参数</strong></p>
<p>首先导入必须的库以及定义一些参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数和数据集</span></span><br><span class="line">input_size = <span class="number">5</span></span><br><span class="line">output_size = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">30</span></span><br><span class="line">data_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>这里主要定义网络输入大小和输出大小，<code>batch</code>以及图片的大小。定义一个<code>device</code>对象。</p>
<p><strong>构建一个假数据集</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RandomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, length</span>):</span><br><span class="line">        self.<span class="built_in">len</span> = length</span><br><span class="line">        self.data = torch.randn(length, size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line"></span><br><span class="line">rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),</span><br><span class="line">                         batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><strong>简单的模型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="comment"># Our model</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(input_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.fc(<span class="built_in">input</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tIn Model: input size&quot;</span>, <span class="built_in">input</span>.size(),</span><br><span class="line">              <span class="string">&quot;output size&quot;</span>, output.size())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<p><strong>创建模型和数据平行</strong></p>
<p>这是核心内容。首先需要定义一个模型示例，并且检查是否有多个GPU，如果有的话就可以将模型包裹在<code>nn.DataParallel</code>，调用<code>model.to(device)</code>。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = Model(input_size, output_size)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Let&#x27;s use&quot;</span>, torch.cuda.device_count(), <span class="string">&quot;GPUs!&quot;</span>)</span><br><span class="line">    <span class="comment"># dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs</span></span><br><span class="line">    model = nn.DataParallel(model)</span><br><span class="line">   </span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>

<p><strong>运行模型</strong></p>
<p>接下来就是运行模型，看看打印信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    <span class="built_in">input</span> = data.to(device)</span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Outside: input size&quot;</span>, <span class="built_in">input</span>.size(),</span><br><span class="line">          <span class="string">&quot;output_size&quot;</span>, output.size())</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In Model: <span class="built_in">input</span> size torch.Size([<span class="number">15</span>, <span class="number">5</span>]) output size torch.Size([<span class="number">15</span>, <span class="number">2</span>])</span><br><span class="line">        In Model: <span class="built_in">input</span> size torch.Size([<span class="number">15</span>, <span class="number">5</span>]) output size torch.Size([<span class="number">15</span>, <span class="number">2</span>])</span><br><span class="line">Outside: <span class="built_in">input</span> size torch.Size([<span class="number">30</span>, <span class="number">5</span>]) output_size torch.Size([<span class="number">30</span>, <span class="number">2</span>])</span><br><span class="line">        In Model: <span class="built_in">input</span> size torch.Size([<span class="number">15</span>, <span class="number">5</span>]) output size torch.Size([<span class="number">15</span>, <span class="number">2</span>])</span><br><span class="line">        In Model: <span class="built_in">input</span> size torch.Size([<span class="number">15</span>, <span class="number">5</span>]) output size torch.Size([<span class="number">15</span>, <span class="number">2</span>])</span><br><span class="line">Outside: <span class="built_in">input</span> size torch.Size([<span class="number">30</span>, <span class="number">5</span>]) output_size torch.Size([<span class="number">30</span>, <span class="number">2</span>])</span><br><span class="line">        In Model: <span class="built_in">input</span> size torch.Size([<span class="number">15</span>, <span class="number">5</span>]) output size torch.Size([<span class="number">15</span>, <span class="number">2</span>])</span><br><span class="line">        In Model: <span class="built_in">input</span> size torch.Size([<span class="number">15</span>, <span class="number">5</span>]) output size torch.Size([<span class="number">15</span>, <span class="number">2</span>])</span><br><span class="line">Outside: <span class="built_in">input</span> size torch.Size([<span class="number">30</span>, <span class="number">5</span>]) output_size torch.Size([<span class="number">30</span>, <span class="number">2</span>])</span><br><span class="line">        In Model: <span class="built_in">input</span> size torch.Size([<span class="number">5</span>, <span class="number">5</span>]) output size torch.Size([<span class="number">5</span>, <span class="number">2</span>])</span><br><span class="line">        In Model: <span class="built_in">input</span> size torch.Size([<span class="number">5</span>, <span class="number">5</span>]) output size torch.Size([<span class="number">5</span>, <span class="number">2</span>])</span><br><span class="line">Outside: <span class="built_in">input</span> size torch.Size([<span class="number">10</span>, <span class="number">5</span>]) output_size torch.Size([<span class="number">10</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>

<p><strong>运行结果</strong></p>
<p>如果仅仅只有1个或者没有GPU，那么<code>batch=30</code>的时候，模型会得到输入输出的大小都是30，但是有多个GPU有如下结论：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2GPU</span></span><br><span class="line"><span class="comment"># on 2 GPUs</span></span><br><span class="line">Let<span class="string">&#x27;s use 2 GPUs!</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span></span><br><span class="line"><span class="string">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span></span><br><span class="line"><span class="string">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])</span></span><br><span class="line"><span class="string">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])</span></span><br><span class="line"><span class="string">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Let<span class="string">&#x27;s use 3 GPUs!</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span></span><br><span class="line"><span class="string">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span></span><br><span class="line"><span class="string">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])</span></span><br><span class="line"><span class="string">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span></span><br><span class="line"><span class="string">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Let<span class="string">&#x27;s use 8 GPUs!</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span></span><br><span class="line"><span class="string">Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span></span><br><span class="line"><span class="string">    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])</span></span><br><span class="line"><span class="string">Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])</span></span><br></pre></td></tr></table></figure>

<p><strong>总结</strong></p>
<p><code>DataParallel</code>会自动分割数据集并发送任务给多个GPU上的模型，然后等每个模型都完成自己的工作，它又会收集融合结果，然后返回。</p>
<h2 id="9-模型的保存、构造网络的简单方法、动态图"><a href="#9-模型的保存、构造网络的简单方法、动态图" class="headerlink" title="9 模型的保存、构造网络的简单方法、动态图"></a>9 模型的保存、构造网络的简单方法、动态图</h2><p><strong>模型的保存和读取</strong></p>
<ul>
<li><p>方法1</p>
<ul>
<li><pre><code class="python"># 保存模型,model为模型的变量，比如net也算
torch.save(model,  &quot;./model_save&quot;)
# 读取
model = torch.load(&quot;./model_save&quot;)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  - 该模型需要构造模型的类出现，否则将报错。</span><br><span class="line"></span><br><span class="line">- 方法2(推荐)</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    # 模型保存方式2:将模型保存为字典参数，内存小</span><br><span class="line">    torch.save(model.state_dict(), &quot;./model_save&quot;)</span><br><span class="line">    # 模型的加载方式2</span><br><span class="line">    vgg16_model2 = torchvision.models.vgg16(pretrained=False) # 先构造一个模型</span><br><span class="line">    vgg16_model2.load_state_dict(torch.load(&quot;./model_save&quot;)) # 参数加载</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
<p><strong>构造网络的简单方法</strong></p>
<p>比如我们有一个模型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mynn</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Mynn, self).__init__()</span><br><span class="line">        <span class="comment"># # 第一个图变第二个图</span></span><br><span class="line">        <span class="comment"># self.conv1 = Conv2d(3, 32, 5, padding=2)</span></span><br><span class="line">        <span class="comment"># # 第二个图变第三个图</span></span><br><span class="line">        <span class="comment"># self.maxpool1 = MaxPool2d(2)</span></span><br><span class="line">        <span class="comment"># # 第三个图变第四个图</span></span><br><span class="line">        <span class="comment"># self.conv2 = Conv2d(32, 32, 5, padding=2)</span></span><br><span class="line">        <span class="comment"># # 第四张图变第五张图</span></span><br><span class="line">        <span class="comment"># self.maxpool2 = MaxPool2d(2)</span></span><br><span class="line">        <span class="comment"># # 第五张图变第六张图</span></span><br><span class="line">        <span class="comment"># self.conv3 = Conv2d(32, 64, 5, padding=2)</span></span><br><span class="line">        <span class="comment"># # 第六张图变第七张图</span></span><br><span class="line">        <span class="comment"># self.maxpool3 = MaxPool2d(2)</span></span><br><span class="line">        <span class="comment"># # 第七张图变第八张图</span></span><br><span class="line">        <span class="comment"># self.flatten1 = Flatten()</span></span><br><span class="line">        <span class="comment"># # 第八张图变第九张图</span></span><br><span class="line">        <span class="comment"># self.linear1 = Linear(1024, 64)</span></span><br><span class="line">        <span class="comment"># self.linear2 = Linear(64, 10)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = self.conv1(x)</span></span><br><span class="line">        <span class="comment"># x = self.maxpool1(x)</span></span><br><span class="line">        <span class="comment"># x = self.conv2(x)</span></span><br><span class="line">        <span class="comment"># x = self.maxpool2(x)</span></span><br><span class="line">        <span class="comment"># x = self.conv3(x)</span></span><br><span class="line">        <span class="comment"># x = self.maxpool3(x)</span></span><br><span class="line">        <span class="comment"># x = self.flatten1(x)</span></span><br><span class="line">        <span class="comment"># x = self.linear1(x)</span></span><br><span class="line">        <span class="comment"># x = self.linear2(x)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>上述模型的构造较为复杂，我们可以将其写为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mynn</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Mynn, self).__init__()</span><br><span class="line">        <span class="comment"># 利用sequential进行模型规范</span></span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>这样就可以实现模型的简化。</p>
<p><strong>动态图</strong></p>
<p>动态图可以通过李沐老师的代码实现，在下文学习实践中的0章节，我们也可以通过另一种方法实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个实例</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"><span class="comment"># 图片地址，训练的话直接用训练的图片</span></span><br><span class="line">image_path = <span class="string">&quot;D:\\LearnAi\\Ai03_pytorch\\TensorBoard\\train\\ants_image\\28847243_e79fe052cd.jpg&quot;</span></span><br><span class="line"><span class="comment"># PIL格式图片</span></span><br><span class="line">img_PIL = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line"><span class="comment"># numpy.array格式图片</span></span><br><span class="line">img_array = np.array(img_PIL)</span><br><span class="line"><span class="comment"># 读取图片</span></span><br><span class="line">writer.add_image(<span class="string">&quot;test1&quot;</span>, img_array, <span class="number">1</span>, dataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y=3x&quot;</span>, <span class="number">3</span>*i, i)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>上述代码可以加入训练过程中实时观看训练的结果。</p>
<h2 id="10-一些经验"><a href="#10-一些经验" class="headerlink" title="10 一些经验"></a>10 一些经验</h2><ul>
<li>我们的模型并非越大越好，模型越大，我们的模型学习能力可能很强，那么可能会把噪声模型等也学会，那么泛化能力就较差，我们的模型要泛化性较好，防止过拟合。</li>
<li></li>
</ul>
<h1 id="Pytorch深度学习实践"><a href="#Pytorch深度学习实践" class="headerlink" title="Pytorch深度学习实践"></a>Pytorch深度学习实践</h1><h2 id="0-Pyotrch动态图的实现"><a href="#0-Pyotrch动态图的实现" class="headerlink" title="0 Pyotrch动态图的实现"></a>0 <code>Pyotrch</code>动态图的实现</h2><p>实现动态图的代码以及教程参考李沐的书籍–动手学习深度学习。</p>
<ul>
<li><p>首先我们需要了解一下<code>ipython</code>的<code>display</code>函数的使用，它应用在<code>Jupyter Notebook</code>中进行图像化表示或其它格式化输出，例如图像、音频、视频、<code>HTML</code>等。</p>
<ul>
<li><p>基本用法：</p>
</li>
<li><pre><code class="python">from IPython import display
display()
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 以一个例子来介绍</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  from IPython import display</span><br><span class="line">  import pandas as pd</span><br><span class="line">  </span><br><span class="line">  # 显示一个字符串</span><br><span class="line">  display.display(&#x27;Hello World&#x27;)</span><br><span class="line">  </span><br><span class="line">  # 显示一个pandas数据</span><br><span class="line">  df = pd.DataFrame(&#123;&#x27;A&#x27;: [1, 2, 3], &#x27;B&#x27;: [4, 5, 6]&#125;)</span><br><span class="line">  display.display(df)</span><br><span class="line">  </span><br><span class="line">  # 显示一个图片</span><br><span class="line">  from PIL import Image</span><br><span class="line">  img = Image.open(&#x27;figpath.png&#x27;)</span><br><span class="line">  display.display(img)</span><br><span class="line">  </span><br><span class="line">  # 显示一段HTML代码</span><br><span class="line">  display.display(&#x27;&lt;h1&gt;欢迎报考东北大学&lt;/h1&gt;&#x27;)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p>最终代码(可封装为函数)</p>
<ul>
<li><pre><code class="python">&quot;&quot;&quot;jupyter需要加上%matplotlib inline
推荐参数：
xlim = [1, epochs]
ylim = [0, 1]
&quot;&quot;&quot;
from IPython import display
import matplotlib.pyplot as plt


def use_svg_display():
    &quot;&quot;&quot;用矢量图显示&quot;&quot;&quot;
    display.set_matplotlib_formats(&#39;svg&#39;)


def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
    &quot;&quot;&quot;坐标轴的设置&quot;&quot;&quot;
    axes.set_xlabel(xlabel)
    axes.set_ylabel(ylabel)
    axes.set_xscale(xscale)
    axes.set_yscale(yscale)
    axes.set_xlim(xlim)
    axes.set_ylim(ylim)
    if legend:
        axes.legend(legend)
    axes.grid()


class Animator:
    &quot;&quot;&quot;在动画中绘制数据&quot;&quot;&quot;

    def __init__(self, xlabel=None, ylabel=None, legend=None,
                 xlim=None, ylim=None, xscale=&#39;linear&#39;, yscale=&#39;linear&#39;,
                 fmts=(&#39;-&#39;, &#39;m--&#39;, &#39;g-&#39;, &#39;r:&#39;), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # 增量的绘制多条线
        if legend is None:
            legend = []
        # 用矢量图显示
        use_svg_display()
        # 或画窗口
        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            # 将其变为矩阵
            self.axes = [self.axes, ]
        # 设置图片参数
        self.X, self.Y, self.fmts = None, None, fmts
        self.config_axes = lambda: set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale,
                                            legend)

    def add(self, x, y):
        &quot;&quot;&quot;向图表中添加多个数据点&quot;&quot;&quot;
        # 选取Y的长度
        # 实际上求解其有几条曲线
        if not hasattr(y, &#39;__len__&#39;):
            y = [y]
        n = len(y)
        # 获取x的长度用于构建坐标轴，一个y值对应一个x值，一张图的x都是一样的，所以快捷的复制n个。
        # 简单说就是让y的长度与x等长。
        if not hasattr(x, &#39;__len__&#39;):
            x = [x] * n
        # 首次使用该函数进行值的初始化。
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        # 将值进行分配到X,Y变量。
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        # 清除当前坐标轴。
        self.axes[0].cla()
        # 画图。
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        # 设置参数。
        self.config_axes()
        # 打印。
        display.display(self.fig)
        # 等待全部显示。
        display.clear_output(wait=True)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  - 上述代码可以直接复制使用。</span><br><span class="line"></span><br><span class="line">- 在基础上可以输出静态图代码：</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    &quot;&quot;&quot;jupyter需要加上%matplotlib inline</span><br><span class="line">    推荐参数：</span><br><span class="line">    xlim = [1, epochs]</span><br><span class="line">    ylim = [0, 1]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    from IPython import display</span><br><span class="line">    import matplotlib.pyplot as plt</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    def use_svg_display():</span><br><span class="line">        &quot;&quot;&quot;用矢量图显示&quot;&quot;&quot;</span><br><span class="line">        display.set_matplotlib_formats(&#x27;svg&#x27;)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):</span><br><span class="line">        &quot;&quot;&quot;坐标轴的设置&quot;&quot;&quot;</span><br><span class="line">        axes.set_xlabel(xlabel)</span><br><span class="line">        axes.set_ylabel(ylabel)</span><br><span class="line">        axes.set_xscale(xscale)</span><br><span class="line">        axes.set_yscale(yscale)</span><br><span class="line">        axes.set_xlim(xlim)</span><br><span class="line">        axes.set_ylim(ylim)</span><br><span class="line">        if legend:</span><br><span class="line">            axes.legend(legend)</span><br><span class="line">        axes.grid()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    class Animator:</span><br><span class="line">        &quot;&quot;&quot;在动画中绘制数据&quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">        def __init__(self, xlabel=None, ylabel=None, legend=None,</span><br><span class="line">                     xlim=None, ylim=None, xscale=&#x27;linear&#x27;, yscale=&#x27;linear&#x27;,</span><br><span class="line">                     fmts=(&#x27;-&#x27;, &#x27;m--&#x27;, &#x27;g-&#x27;, &#x27;r:&#x27;), nrows=1, ncols=1,</span><br><span class="line">                     figsize=(3.5, 2.5)):</span><br><span class="line">            # 增量的绘制多条线</span><br><span class="line">            if legend is None:</span><br><span class="line">                legend = []</span><br><span class="line">            # 用矢量图显示</span><br><span class="line">            use_svg_display()</span><br><span class="line">            # 或画窗口</span><br><span class="line">            self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">            if nrows * ncols == 1:</span><br><span class="line">                # 将其变为矩阵</span><br><span class="line">                self.axes = [self.axes, ]</span><br><span class="line">            # 设置图片参数</span><br><span class="line">            self.X, self.Y, self.fmts = None, None, fmts</span><br><span class="line">            self.config_axes = lambda: set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale,</span><br><span class="line">                                                legend)</span><br><span class="line">    </span><br><span class="line">        def add(self, x, y, is_change=True, is_narray=False):</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">            向图表中添加多个数据点</span><br><span class="line">            is_narray 表示静态画图是否是多维，默认为1维</span><br><span class="line">            is_change 表示是否为动态画图，默认为动态</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">            if is_change:</span><br><span class="line">                # 选取Y的长度</span><br><span class="line">                # 实际上求解其有几条曲线</span><br><span class="line">                if not hasattr(y, &#x27;__len__&#x27;):</span><br><span class="line">                    y = [y]</span><br><span class="line">                n = len(y)</span><br><span class="line">                # 获取x的长度用于构建坐标轴，一个y值对应一个x值，一张图的x都是一样的，所以快捷的复制n个。</span><br><span class="line">                # 简单说就是让y的长度与x等长。</span><br><span class="line">                if not hasattr(x, &#x27;__len__&#x27;):</span><br><span class="line">                    x = [x] * n</span><br><span class="line">                # 首次使用该函数进行值的初始化。</span><br><span class="line">                if not self.X:</span><br><span class="line">                    self.X = [[] for _ in range(n)]</span><br><span class="line">                if not self.Y:</span><br><span class="line">                    self.Y = [[] for _ in range(n)]</span><br><span class="line">                # 将值进行分配到X,Y变量。</span><br><span class="line">                for i, (a, b) in enumerate(zip(x, y)):</span><br><span class="line">                    if a is not None and b is not None:</span><br><span class="line">                        self.X[i].append(a)</span><br><span class="line">                        self.Y[i].append(b)</span><br><span class="line">                # 清除当前坐标轴。</span><br><span class="line">                self.axes[0].cla()</span><br><span class="line">                # 画图。</span><br><span class="line">                for x, y, fmt in zip(self.X, self.Y, self.fmts):</span><br><span class="line">                    self.axes[0].plot(x, y, fmt)</span><br><span class="line">                # 设置参数。</span><br><span class="line">                self.config_axes()</span><br><span class="line">                # 打印。</span><br><span class="line">                display.display(self.fig)</span><br><span class="line">                # 等待全部显示。</span><br><span class="line">                display.clear_output(wait=True)</span><br><span class="line">            else:</span><br><span class="line">                if is_narray:</span><br><span class="line">                    n = len(y)</span><br><span class="line">                    self.Y = y</span><br><span class="line">                else:</span><br><span class="line">                    n = 1</span><br><span class="line">                    self.Y = [y]</span><br><span class="line">                # 获取数据</span><br><span class="line">                self.X = x</span><br><span class="line">                # 清空</span><br><span class="line">                self.axes[0].cla()</span><br><span class="line">                # 画图</span><br><span class="line">                for i in range(n):</span><br><span class="line">                    self.axes[0].plot(self.X, self.Y[i], self.fmts[i])</span><br><span class="line">                # 设置参数</span><br><span class="line">                self.config_axes()</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1 Overview"></a>1 Overview</h2><ul>
<li><p>人类智能</p>
<ul>
<li>在学校，中午吃什么就是一个决策问题，做决策的过程就是推理；推理需要输入信息，各种输入信息决定输出。这种推理过程可以理解为人类智能。</li>
<li>根据一些信息如照片进行预测的过程也可以称为人类智能。</li>
</ul>
</li>
<li><p>机器学习</p>
<ul>
<li>对应于人类智能，只是用来做推理的换成算法。如中午吃什么可以根据算法进行推算得到结果。</li>
</ul>
</li>
<li><p>监督学习</p>
<ul>
<li>我们将数据集打上标签，再将这些数据丢入模型进行训练，将训练的结果与标签进行对比的算法。</li>
<li>机器学习的算法来自于数据，并非人工设定(炼丹！！！！)。</li>
</ul>
</li>
<li><p>AI、机器学习、深度学习的关系图</p>
<ul>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230609221643593.png" alt="image-20230609221643593"></li>
</ul>
</li>
<li><p>学习系统的开发过程：</p>
<ul>
<li>基于规则系统(Rule-based systems)：输入(Input)—-&gt;手工设计程序(Hand-designed program)—-&gt;输出(Output)。最早的人工智能算法基于该系统。</li>
<li>经典机器学习(Classic machine learning)：输入(Input)—-&gt;手工设计提取特征(Hand-designed features)—-&gt;建立输出与特征的映射函数(Mapping from features)，简单理解就是f(x)表达式—-&gt;输出(Output)</li>
<li>表示学习(Representation learning)：输入(Input)—-&gt;特征提取(此处希望是智能提取而非人工，所以也作为一个过程)(features)—-&gt;建立输出与特征的映射函数(Mapping from features)，简单理解就是f(x)表达式—-&gt;输出(Output)。表示学习一般是学到高维空间降到低位空间的表示。<ul>
<li>表示学习的一个分支：Manifold(高维空间的低位流形)：比如将三维降低为二维。</li>
</ul>
</li>
<li>深度学习(Deep Learning)：输入(Input)—-&gt;简单特征(Simple features)(如图片像素点，声音序列)—-&gt;设置额外具体的层用来提取特征(Additional layers of more abstract features)—-&gt;建立输出与特征的映射关系(Mapping from features)(深度学习里面可以是多层神经网络)—-&gt;输出(Output)<ul>
<li>表示学习的特征和学习器(Mapping from features)是分开的，但是深度学习这一块是统一的，所以深度学习也叫端到端的过程</li>
</ul>
</li>
</ul>
</li>
<li><p>维度诅咒</p>
<ul>
<li><p>如果输入，也就是features的数量越多，如下图是两个features：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230609224208142.png" alt="image-20230609224208142"></p>
</li>
<li><p>对于一个features，比如我们抽样10点，那么数据就是10，上图是features为2，我们继续抽样10个点，那么x和y都要抽10个点，总共抽样就是100个点。</p>
</li>
<li><p>如果三维，那我们就是需要10^3^个数据，所以维度越大，我们需要的数据越多，所以会出现维度诅咒。</p>
</li>
<li><p>为了降低数据需求，可以采取让<code>N</code>维空间映射到低维空间，如果采用线性映射可以理解为线性代数，有一个N维空间，用矩阵表示为：<code>N x 1</code>，我们需要将其映射到三维空间，只需要找到一个大小为<code>3 x N</code>的矩阵相乘即可，公式如下：</p>
</li>
<li><p>$$<br>\left[ \begin{array}{l}<br>m_{11}&amp;		m_{12}&amp;		\cdots&amp;		m_{1n}\<br>m_{21}&amp;		m_{22}&amp;		\cdots&amp;		m_{2n}\<br>m_{31}&amp;		m_{3n}&amp;		\cdots&amp;		m_{3n}\<br>\end{array} \right] \left[ \begin{array}{l}<br>x_1\<br>x_2\<br>\vdots\<br>x_n\<br>\end{array} \right] &#x3D;\left[ \begin{array}{c}<br>y_1\<br>y_2\<br>y_3\<br>\end{array} \right]<br>$$</p>
</li>
<li><p>前面的<code>3 x N</code>矩阵的寻找需要尽量保证转换后的结果与高位度的结果类似才行，不可以随便找。</p>
</li>
</ul>
</li>
<li><p>SVM 受到的挑战</p>
<ul>
<li>人工提取特征数据受到很多限制。</li>
<li>处理大数据集效果不是很好。</li>
<li>越来越多的应用需要处理无结构的数据。</li>
</ul>
</li>
<li><p>反向传播</p>
<ul>
<li><p>深度学习求梯度直接算解析式似乎是一个不可行的方法，模型复杂的时候，解析表达式将非常复杂；为了得到梯度应该采取求解反向传播的方法。</p>
</li>
<li><p>反向传播的核心是计算图。</p>
</li>
<li><p>如下计算图：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230610161540436.png" alt="image-20230610161540436"></p>
</li>
<li><p>在计算图中只能进行一些原子计算。</p>
<ul>
<li><p>原子计算：我们不再将运算进行分割的运算称为原子计算。如矩阵乘法以及卷积运算都是基本的原子计算。</p>
</li>
<li><p>上图中，对于<code>c</code>来说，它的原子计算是<code>+</code>法，对于<code>d</code>来说也是。对于<code>e</code>来说它的原子计算是乘法。</p>
</li>
</ul>
</li>
<li><p>上图一步一步往上求，最后得到<code>e</code>的结果的过程称为前馈过程。</p>
</li>
<li><p>令上述<code>b=2</code>，<code>a=1</code>在前馈过程的时候，如<code>c=a+b</code>的时候，我们在这个节点就可以同时对<code>a</code>和<code>b</code>求偏导。明显可以看到其结果为1。同理，我们在节点<code>d</code>可以求<code>d</code>对<code>b</code>的偏导，结果应该也是1。然后我们再到节点<code>e</code>，我们求<code>e</code>对<code>c</code>的偏导，其结果为<code>d=3</code>。我们继续求<code>e</code>对<code>d</code>的偏导，其结果为<code>c=3</code>。此时我们最终目标是求<code>e</code>对<code>a</code>的倒数和<code>e</code>对<code>b</code>的导数。那我们就应该把<code>a</code>到<code>e</code>路径上所有对<code>a</code>的偏导数相乘：3 * 1 &#x3D; 3，对<code>b</code>同理，但是<code>b</code>有两条路，我们需要将这两条路的结果求和：1 * 3 + 1 * 3 &#x3D; 6。</p>
</li>
</ul>
</li>
</ul>
<h2 id="2-线性模型"><a href="#2-线性模型" class="headerlink" title="2 线性模型"></a>2 线性模型</h2><ul>
<li><p>案例一</p>
<ul>
<li><p>假定学生成绩和花费的时间有如下关系：</p>
</li>
<li><table>
<thead>
<tr>
<th align="center">x(hours)</th>
<th align="center">y(points)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">4</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">6</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">?</td>
</tr>
</tbody></table>
</li>
<li><p>这个标给出时间以及结果的就是模型训练该做的，没给的部分就是在推理过程用或者预测过程用。</p>
</li>
<li><p>这种学习叫做监督学习。</p>
</li>
</ul>
</li>
<li><p>泛化</p>
<ul>
<li>在训练集效果好，测试集也要有好效果。</li>
<li>在拿不到测试集的时候，为了减小过拟合，提高泛化能力，应该拿一部分出来作为开发集用于测试集。</li>
</ul>
</li>
<li><p>模型设计</p>
<ul>
<li><p>线性模型：</p>
</li>
<li><p>$$<br>y&#x3D;\omega *x+b<br>$$</p>
</li>
<li><p>简化模型：</p>
</li>
<li></li>
<li><p>$$<br>y&#x3D;\omega ,,* x<br>$$</p>
</li>
<li><p>我们实际上需要找到合适的权重<code>w</code>使得与真实值相差不大，这里相差不大的衡量得用评估模型。</p>
</li>
</ul>
</li>
<li><p>评估模型</p>
<ul>
<li><p>评估模型就是损失函数，一般采用如下评估模型：</p>
</li>
<li><p>$$<br>loss&#x3D;\left( y-y_{hat} \right) ^2&#x3D;\left( y-w*x \right) ^2<br>$$</p>
</li>
</ul>
</li>
<li><p>平均平方误差(MSE)</p>
<ul>
<li>$$<br>\cos t&#x3D;\frac{1}{N}\sum_{n&#x3D;1}^N{\left( y-y_{hat} \right) ^2}<br>$$</li>
</ul>
</li>
<li><p>模型训练代码如下：</p>
<ul>
<li><pre><code class="python"># 简单实现
import torch
from torch import nn
from torch import optim

# 导入数据
x_data = torch.tensor([1.0, 2.0, 3.0, 4.0], dtype=torch.float32, requires_grad=True).reshape(-1, 1)
y_data = torch.tensor([2.0, 4.0, 6.0, 8.0], dtype=torch.float32).reshape(-1, 1)
# 训练设备
device = torch.device(&#39;cuda:0&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;)

# 构建网络
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(1 ,1)

    def forward(self, x):
        return self.fc1(x)

net = Net()

loss = nn.MSELoss()

epochs, lr = 10, 1e-2
net = net.to(device)
# 优化函数采用随机梯度下降
optimer = optim.SGD(net.parameters(), lr=lr)
for epoch in range(epochs):
    loss_sum = 0
    for x, y in zip(x_data, y_data):
        # 获取数据
        features, labels = x.to(device), y.to(device)
        # 清空梯度
        optimer.zero_grad()
        # 计算输出值
        y_hat = net(features)
        # 计算损失函数
        loss_num = loss(labels, y_hat)
        # 求梯度
        loss_num.backward()
        # 更新参数
        optimer.step()
        loss_sum += loss_num
    # 打印结果
    print(f&#39;第&#123;epoch + 1&#125;轮训练，损失为&#123;loss_sum&#125;&#39;)
print(net.fc1.weight)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 画图代码如下：</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    import torch</span><br><span class="line">    import numpy as np</span><br><span class="line">    import matplotlib.pyplot as plt</span><br><span class="line">    from torch import nn</span><br><span class="line">    </span><br><span class="line">    # 导入数据</span><br><span class="line">    x_data = [1.0, 2.0, 3.0]</span><br><span class="line">    y_data = [2.0, 4.0, 6.0]</span><br><span class="line">    </span><br><span class="line">    def forward(x, w, b):</span><br><span class="line">        return w * x + b</span><br><span class="line">    </span><br><span class="line">    # 损失函数采用MSE</span><br><span class="line">    def my_loss(y, y_hat):</span><br><span class="line">        return (y - y_hat) ** 2</span><br><span class="line">    </span><br><span class="line">    w_list = []</span><br><span class="line">    loss_list = []</span><br><span class="line">    b_list = []</span><br><span class="line">    for w in np.arange(0.0, 4.0, 0.5):</span><br><span class="line">        for b in np.arange(-2.0, 2.0, 0.5):</span><br><span class="line">            print(&#x27;w=&#x27;, w)</span><br><span class="line">            print(&#x27;\t&#x27;, &#x27;b=&#x27;, b)</span><br><span class="line">            l_sum = 0</span><br><span class="line">            for x, y in zip(x_data, y_data):</span><br><span class="line">                y_hat = forward(x, w, b)</span><br><span class="line">                loss_num = my_loss(y, y_hat)</span><br><span class="line">                l_sum += loss_num</span><br><span class="line">                print(&#x27;\t\t&#x27;, x, y, y_hat, loss_num, )</span><br><span class="line">            print(f&#x27;loss:&#123;l_sum&#125;&#x27;)</span><br><span class="line">            loss_list.append(l_sum)</span><br><span class="line">            b_list.append(b)</span><br><span class="line">            w_list.append(w)</span><br><span class="line">    </span><br><span class="line">    # 绘图</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    axe = fig.add_subplot(1, 1, 1, projection=&#x27;3d&#x27;)</span><br><span class="line">    axe.plot_trisurf(w_list, b_list, loss_list, cmap=&#x27;rainbow&#x27;)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230611222406280.png" alt="image-20230611222406280"></p>
</li>
</ul>
</li>
</ul>
<h2 id="3-梯度下降法"><a href="#3-梯度下降法" class="headerlink" title="3 梯度下降法"></a>3 梯度下降法</h2><ul>
<li><p>梯度下降法介绍</p>
<ul>
<li><p>梯度下降算法就是逐步接近目标点，也就是最佳得权重的算法。比如我们有一个函数：<code>cos(t)</code>，我们初始点出现在下图红色的点的位置。</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230611223528276.png" alt="image-20230611223528276"></p>
</li>
<li><p>我们可以计算该点的梯度：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230611223604249.png" alt="image-20230611223604249"></p>
</li>
<li><p>也就是下面公式：</p>
</li>
<li><p>$$<br>\frac{\partial \cos \left( t \right)}{\partial \omega}<br>$$</p>
</li>
<li><p>上述图片的<code>x</code>轴为<code>w</code>。</p>
</li>
<li><p>上图红色点导数值大于0，说明随着<code>w</code>的增加，函数值在上升，反之。我们这里的最优目标在最下面的点，那我们不能允许函数值再升高，我们需要选择更小的<code>w</code>来使得函数中下降，因为该点导数大于零，想下降就降低<code>w</code>的值。</p>
</li>
<li><p>所以我们更新权重的方法：</p>
</li>
<li><p>$$<br>\omega &#x3D;\omega -\alpha \frac{\partial \cos \left( t \right)}{\partial \omega}<br>$$</p>
</li>
<li><p>这样可以保证往最小值去走。上图中梯度前面的系数<code>afa</code>一般就是训练的步长或者学习率。</p>
</li>
<li><p>这个算法其实也是会进入局部最优解的，这个和步长其实有很大关系。(步长设置，炼丹！！！)</p>
</li>
<li><p>神经网络之所以可以用，是因为经过大量的训练以及研究人员发现，神经网络实际上没有太多的局部最优解的存在，但是它有一个特殊点叫鞍点。</p>
</li>
<li><p>鞍点：鞍点就是当我们对该点求导时梯度为0。</p>
</li>
<li><p>可以看出，当我们目标点处在鞍点的时候，我们将无法进行迭代操作。因为鞍点处梯度为0，所以每一步更新都没有发生变化。</p>
</li>
<li><p>上一节课我们有损失函数(之所以用损失函数是因为我们的损失函数最小值对应得权重值是最优的)表达式如下：</p>
</li>
<li><p>$$<br>\frac{1}{N}\sum_{n&#x3D;1}^N{\left( x_n\cdot \omega +y_n \right) ^2}<br>$$</p>
</li>
<li><p>我们可以求解它的梯度：</p>
</li>
<li><p>$$<br>\frac{\partial}{\partial \omega}\frac{1}{N}\sum_{n&#x3D;1}^N{\left( x_n\cdot \omega -y_n \right) ^2}<br>\<br>&#x3D;\frac{1}{N}\sum_{n&#x3D;1}^N{\frac{\partial}{\partial \omega}\left( x_n\cdot \omega -y_n \right) ^2}<br>\<br>&#x3D;\frac{1}{N}\sum_{n&#x3D;1}^N{2\cdot \left( x_n\cdot \omega -y_n \right) \cdot x_n}<br>$$</p>
</li>
<li><p>所以更新的过程如下：</p>
</li>
<li><p>$$<br>\omega &#x3D;\omega -\alpha \frac{1}{N}\sum_{n&#x3D;1}^N{2\cdot \left( x_n\cdot \omega -y_n \right) \cdot x_n}<br>$$</p>
</li>
</ul>
</li>
<li><p>具体代码的实现</p>
<ul>
<li><pre><code class="python"># 数据集
x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

# 初始权重
w = 15.0

# 定义线性
def forward(x):
    return w * x

# 定义损失函数
def cost(xs, ys):
    cost_sum = 0
    for x, y in zip(xs, ys):
        y_hat = forward(x)
        cost_sum += (y - y_hat) ** 2
    return cost_sum / len(xs)

# 梯度函数
def gradient(xs, ys):
    grad = 0
    for x, y in zip(xs, ys):
        grad += 2 * (x * w - y) * x
    return grad / len(xs)

# 打印训练前的输出
print(&#39;Predict (before training)&#39;, 4, forward(4))

# 开始训练
epochs, lr = 10, 1e-1
epoch_sum = []
loss_sum = []
for epoch in range(epochs):
    # 计算损失：
    loss = cost(x_data, y_data)
    # 更新梯度
    w = w - lr * gradient(x_data, y_data)
    print(f&#39;Predict training(&#123;epoch + 1&#125;)&#39;, 4, forward(4))
    epoch_sum.append(epoch)
    loss_sum.append(loss)
print(&#39;w=&#39;, w)

# 画图
import matplotlib
import matplotlib.pyplot as plt

fig = plt.figure()
axe = fig.add_subplot(1, 1, 1)
axe.plot(epoch_sum, loss_sum)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 随机梯度下降</span><br><span class="line"></span><br><span class="line">  - 梯度下降我们采取整个损失的平均梯度作为梯度。随机梯度下降我们是从所有数据中随机的选取一个数据作为梯度。</span><br><span class="line"></span><br><span class="line">  - $$</span><br><span class="line">    \omega =\omega -\alpha \frac&#123;\partial loss&#125;&#123;\partial \omega&#125;</span><br><span class="line">    $$</span><br><span class="line"></span><br><span class="line">  - $$</span><br><span class="line">    \frac&#123;\partial loss_n&#125;&#123;\partial \omega&#125;=2\cdot x_n\cdot \left( x_n\omega -y_n \right)</span><br><span class="line">    $$</span><br><span class="line"></span><br><span class="line">  - 随机梯度下降可以有效避免在鞍点出现参数不更新的情况。因为在计算过程中会有噪声，可以使单个计算值并不永远为0，可以有几率跳出鞍点不更新的情况。</span><br><span class="line"></span><br><span class="line">  - 代码如下：</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    # 数据集</span><br><span class="line">    x_data = [1.0, 2.0, 3.0]</span><br><span class="line">    y_data = [2.0, 4.0, 6.0]</span><br><span class="line">    </span><br><span class="line">    # 初始权重</span><br><span class="line">    w = 15.0</span><br><span class="line">    </span><br><span class="line">    # 定义线性</span><br><span class="line">    def forward(x):</span><br><span class="line">        return w * x</span><br><span class="line">    </span><br><span class="line">    def loss(x, y):</span><br><span class="line">        y_hat = forward(x)</span><br><span class="line">        return (y - y_hat) ** 2</span><br><span class="line">    </span><br><span class="line">    def gradient(xs, ys):</span><br><span class="line">        return 2 * (xs * w - ys) * xs</span><br><span class="line">    </span><br><span class="line">    # 打印训练前的输出</span><br><span class="line">    print(&#x27;Predict (before training)&#x27;, 4, forward(4))</span><br><span class="line">    </span><br><span class="line">    epochs, lr = 10, 1e-1</span><br><span class="line">    epoch_sum = []</span><br><span class="line">    loss_sum = []</span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        loss_sum0 = 0</span><br><span class="line">        for x, y in zip(x_data, y_data):</span><br><span class="line">            # 计算损失:</span><br><span class="line">            loss_num = loss(x, y)</span><br><span class="line">            # 更新梯度:</span><br><span class="line">            w = w - lr * gradient(x, y)</span><br><span class="line">            loss_sum0 += loss_num</span><br><span class="line">            print(f&#x27;Predict training(&#123;epoch + 1&#125;)&#x27;, 4, forward(4))</span><br><span class="line">        epoch_sum.append(epoch)</span><br><span class="line">        loss_sum.append(loss_sum0 / len(x_data))</span><br><span class="line">    print(&#x27;w=&#x27;, w)</span><br><span class="line">    </span><br><span class="line">    # 画图</span><br><span class="line">    import matplotlib</span><br><span class="line">    import matplotlib.pyplot as plt</span><br><span class="line">    </span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    axe = fig.add_subplot(1, 1, 1)</span><br><span class="line">    axe.plot(epoch_sum, loss_sum)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="4-反向传播"><a href="#4-反向传播" class="headerlink" title="4 反向传播"></a>4 反向传播</h2><ul>
<li><p>介绍</p>
<ul>
<li><p>对于下图的神经网络：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230613142034290.png" alt="image-20230613142034290"></p>
</li>
<li><p>我们输入是一个<code>1x5</code>的矩阵，第一次的输出大小是<code>1x6</code>的矩阵，根据线性代数的知识，之间的权重矩阵就是<code>6x5</code>的矩阵，同理第一个输出到第二个输出的权重矩阵大小是<code>7x6</code>的矩阵。我们在这种网络求解梯度将比较复杂。</p>
</li>
<li><p>我们再介绍两层的神经网络。模型如下：</p>
</li>
<li><p>$$<br>\hat{y}&#x3D;\omega _2\left( \omega _1\cdot x+b_1 \right) +b_2<br>$$</p>
</li>
<li><p>计算图如下：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230613142754524.png" alt="image-20230613142754524"></p>
</li>
<li><p>上述绿色部分的<code>MM</code>表示矩阵乘法。</p>
</li>
<li><p>上述的计算图可以发现我们可以将其简化为下面的计算图：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230613143025369.png" alt="image-20230613143025369"></p>
</li>
<li><p>$$<br>\hat{y}&#x3D;\omega _2\left( \omega _1\cdot x+b_1 \right) +b_2<br>\<br>&#x3D;\omega _2\cdot \omega _1\cdot x+\omega _2\cdot b_1+b_2<br>\<br>&#x3D;\omega \cdot x+b<br>$$</p>
</li>
<li><p>这样只要是线性层，我们都可以简化为这样，但是实际中我们这样做会无法构建复杂的模型，我们加的权重将无意义。所以我们要在每个输出加上非线性函数，使得模型不再是线性模型：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230613143413633.png" alt="image-20230613143413633"></p>
</li>
<li><p>加上激活函数，整个模型非线性，无法简化可以构造复杂模型，如果不加，全是线性的模型都可以等效于一个一层的线性模型，无法实现复杂度的提升。</p>
</li>
</ul>
</li>
<li><p>裂式求导法则</p>
<ul>
<li><p>首先需要创建计算图，如一层线性网络：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230613143843287.png" alt="image-20230613143843287"></p>
</li>
<li><p>在上述运算中，我们首先应该做前馈运算，它将沿着箭头方向计算到<code>Loss</code>处。我么再从<code>Loss</code>进行反向运算。</p>
</li>
<li><p>我们的<code>f</code>会计算<code>z</code>关于<code>x</code>以及<code>w</code>的值。我们的<code>Loss</code>处会计算得到<code>Loss</code>关于<code>z</code>的偏导值。</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230613144212790.png" alt="image-20230613144212790"></p>
</li>
<li><p>红色箭头用于算梯度，也就是反向传播。 最终可以得到<code>x</code>和<code>w</code>的导数。</p>
</li>
<li><p>我们以线性模型为例子：</p>
</li>
<li><p>模型与损失函数如下：</p>
</li>
<li><p>$$<br>\hat{y}&#x3D;x\cdot \omega<br>\<br>loss&#x3D;\left( \hat{y}-y \right) ^2&#x3D;\left( x\cdot \omega -y \right) ^2<br>$$</p>
</li>
<li><p>计算图如下：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230613145014184.png" alt="image-20230613145014184"></p>
</li>
<li><p>我们假设<code>x=1</code>和<code>w=1</code>。我们首先进行前馈运算，同时记录每次输出对输入的导数。在<code>pytorch</code>中输入的变量需要自己指定需要梯度。</p>
</li>
<li><p>第一次运算，求导结果为<code>1</code> ，计算结果为<code>1</code></p>
</li>
<li><p>第二次运算，求导结果为<code>1</code>，计算结果为<code>-1</code></p>
</li>
<li><p>第三次运算，求导结果为<code>2*(-1)=-2</code>，计算结果为<code>1</code></p>
</li>
<li><p>我们再进行反向传播得到<code>loss</code>对于<code>w</code>的梯度为：<code>-2*1*1=-2</code></p>
</li>
<li><p>再使用随机梯度下降算法对权重进行更替。</p>
</li>
</ul>
</li>
<li><p>在<code>pytorch</code>计算</p>
<ul>
<li><p>一个<code>tensor</code>变量有两个基本方法，一个是<code>data</code>也就是最基本的本身值，另一个是<code>grad</code>用于保存权重值。</p>
</li>
<li><p>代码如下：</p>
</li>
<li><pre><code class="python">import torch

# 运行设备的选择
device = torch.device(&#39;cuda:0&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;)

# 构建数据集
x_data = torch.tensor([1.0, 2.0, 3.0], device=device)
y_data = torch.tensor([2.0, 4.0, 6.0], device=device)

# 初始化权重
w = torch.tensor([1.0], requires_grad=True, device=device)

# 线性模型
def forward(x):
    return w * x

# 定义损失函数
def loss(x, y):
    y_hat = forward(x)
    return (y - y_hat) ** 2

# 开始测试
epochs, lr = 20, 1e-1
loss_list = []
epoch_list = []
for epoch in range(epochs):
    loss_sum = 0
    for x, y in zip(x_data, y_data):
        x, y = x.to(device), y.to(device)
        # 计算损失
        loss_num = loss(x, y)
        # 计算梯度
        loss_num.backward()
        loss_sum += loss_num
        # 打印w权重
        print(&#39;w.grad =&#39;, w.grad)
        print(loss_num)
        # 更新参数，为了不影响权重要用data去计算与清零。
        w.data = w.data - lr * w.grad.data
        # 清除w的梯度
        w.grad.data.zero_()
    loss_list.append(loss_sum)
    epoch_list.append(epoch + 1)
print(&#39;end w =&#39;, w)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 上述代码的梯度更新是需要用`.data`的，这样不会更新计算图。</span><br><span class="line"></span><br><span class="line">- 上述代码的`loss_sum`也会进入计算图，如下图所示：</span><br><span class="line"></span><br><span class="line">- ![image-20230613155654147](../pictures/pytorch快速入门/image-20230613155654147.png)</span><br><span class="line"></span><br><span class="line">- `loss_sum`也会有梯度信息，我们不要让让它进计算图很简单，取`loss_num.item()`进行求和即可。实际上用`loss_num.data`也可以，如果需要张量形式保存用这个，要用`matlibplot`画图不需要张量保存可以直接用`item()`。</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  import torch</span><br><span class="line">  </span><br><span class="line">  # 运行设备的选择</span><br><span class="line">  device = torch.device(&#x27;cuda:0&#x27;) if torch.cuda.is_available() else torch.device(&#x27;cpu&#x27;)</span><br><span class="line">  </span><br><span class="line">  # 构建数据集</span><br><span class="line">  x_data = torch.tensor([1.0, 2.0, 3.0], device=device)</span><br><span class="line">  y_data = torch.tensor([2.0, 4.0, 6.0], device=device)</span><br><span class="line">  </span><br><span class="line">  # 初始化权重</span><br><span class="line">  w = torch.tensor([1.0], requires_grad=True, device=device)</span><br><span class="line">  </span><br><span class="line">  # 线性模型</span><br><span class="line">  def forward(x):</span><br><span class="line">      return w * x</span><br><span class="line">  </span><br><span class="line">  # 定义损失函数</span><br><span class="line">  def loss(x, y):</span><br><span class="line">      y_hat = forward(x)</span><br><span class="line">      return (y - y_hat) ** 2</span><br><span class="line">  </span><br><span class="line">  # 开始测试</span><br><span class="line">  epochs, lr = 20, 1e-1</span><br><span class="line">  loss_list = []</span><br><span class="line">  epoch_list = []</span><br><span class="line">  for epoch in range(epochs):</span><br><span class="line">      loss_sum = 0</span><br><span class="line">      for x, y in zip(x_data, y_data):</span><br><span class="line">          x, y = x.to(device), y.to(device)</span><br><span class="line">          # 计算损失</span><br><span class="line">          loss_num = loss(x, y)</span><br><span class="line">          # 计算梯度</span><br><span class="line">          loss_num.backward()</span><br><span class="line">          loss_sum += loss_num.item()</span><br><span class="line">          print(&#x27;loss_sum=&#x27;, loss_sum)</span><br><span class="line">          # 打印w权重</span><br><span class="line">          print(&#x27;w.grad =&#x27;, w.grad)</span><br><span class="line">          print(&#x27;loss_num=&#x27;, loss_num)</span><br><span class="line">          # 更新参数，为了不影响权重要用data去计算与清零。</span><br><span class="line">          w.data = w.data - lr * w.grad.data</span><br><span class="line">          # 清除w的梯度</span><br><span class="line">          w.grad.data.zero_()</span><br><span class="line">      loss_list.append(loss_sum)</span><br><span class="line">      epoch_list.append(epoch + 1)</span><br><span class="line">  print(&#x27;end w =&#x27;, w)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
<p>注意：本章课后题模型较小，切勿尝试用大数据去训练小模型。</p>
<h2 id="5-用Pytorch实现线性回归"><a href="#5-用Pytorch实现线性回归" class="headerlink" title="5 用Pytorch实现线性回归"></a>5 用<code>Pytorch</code>实现线性回归</h2><ul>
<li><p>深度神经网络步骤：</p>
<ul>
<li>1.数据集(Prepare dataset)</li>
<li>2.设计网络模型(Design model using Class)</li>
<li>3.构造损失函数以及优化器(construct loss and optimizer)</li>
<li>4.训练(Training cycle)</li>
</ul>
</li>
<li><p>准备数据</p>
<ul>
<li><p>这里开始使用<code>mini-batch</code>也就是一次性把一个<code>batch</code>的数据都求出来。</p>
</li>
<li><p>我们以<code>batch=3</code>进行讲解，我们前面提到模型是<code>wx+b</code>我们现在一次算一个<code>batch</code>，模型的计算过程如下：</p>
</li>
<li><p>$$<br>\left[ \begin{array}{c}<br>\hat{y}_1\<br>\hat{y}_2\<br>\hat{y}_3\<br>\end{array} \right] &#x3D;\omega \left[ \begin{array}{c}<br>\hat{x}_1\<br>\hat{x}_2\<br>\hat{x}_3\<br>\end{array} \right] +b<br>$$</p>
</li>
<li><p>这里的<code>b</code>可以通过广播机制变为一个<code>3x1</code>的矩阵：</p>
</li>
<li><p>$$<br>\left[ \begin{array}{c}<br>\hat{y}_1\<br>\hat{y}_2\<br>\hat{y}_3\<br>\end{array} \right] &#x3D;\omega \left[ \begin{array}{c}<br>\hat{x}_1\<br>\hat{x}_2\<br>\hat{x}_3\<br>\end{array} \right] +\left[ \begin{array}{c}<br>b\<br>b\<br>b\<br>\end{array} \right]<br>$$</p>
</li>
<li><p>继续算损失函数，根据损失函数的公式可以知道有如下计算：</p>
</li>
<li><p>$$<br>\left[ \begin{array}{c}<br>loss_1\<br>loss_2\<br>loss_3\<br>\end{array} \right] &#x3D;\left( \left[ \begin{array}{c}<br>\hat{y}_1\<br>\hat{y}_2\<br>\hat{y}_3\<br>\end{array} \right] +\left[ \begin{array}{c}<br>y_1\<br>y_2\<br>y_3\<br>\end{array} \right] \right) ^2<br>$$</p>
</li>
<li><pre><code class="python"># 加载数据
x_data = torch.tensor([1.0, 2.0, 3.0]).reshape(3, 1)
y_data = torch.tensor([2.0, 4.0, 6.0]).reshape(3, 1)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 设计网络模型</span><br><span class="line"></span><br><span class="line">  - 如果我们的模型pytorch没有，我们可以通过用pytorch现有的模型进行集成，以后进行调用。或者自己定义计算块，继承`Functions`，这个它不会自动来求导。后者暂时用的少。</span><br><span class="line"></span><br><span class="line">  - `__call__`函数会默认调用`forward()`函数，所以我们只需要实现`forward`函数即可在调用类时自动运行而不需要再去实现`__call__`函数。</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    # 构建网络</span><br><span class="line">    class Net(nn.Module):</span><br><span class="line">        def __init__(self):</span><br><span class="line">            super(Net, self).__init__()</span><br><span class="line">            self.fc1 = nn.Linear(1, 1)</span><br><span class="line">    </span><br><span class="line">        def forward(self, x):</span><br><span class="line">            return self.fc1(x)</span><br><span class="line">    </span><br><span class="line">    net = Net()</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p>构造损失函数以及优化器</p>
<ul>
<li><pre><code class="python"># 构造损失函数和优化器
loss = nn.MSELoss(reduction=&#39;sum&#39;)
optimizer = torch.optim.SGD(net.parameters(), lr=1e-2)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 构造训练</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    # 开始训练</span><br><span class="line">    for epoch in range(1000):</span><br><span class="line">        # 计算损失</span><br><span class="line">        y_hat = net(x_data)</span><br><span class="line">        loss_num = loss(y_hat, y_data)</span><br><span class="line">        # 梯度为0</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        # 反向</span><br><span class="line">        loss_num.backward()</span><br><span class="line">        # 更新</span><br><span class="line">        optimizer.step()</span><br><span class="line">        print(loss_num)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="6-逻辑斯蒂回归-分类问题"><a href="#6-逻辑斯蒂回归-分类问题" class="headerlink" title="6 逻辑斯蒂回归(分类问题)"></a>6 逻辑斯蒂回归(分类问题)</h2><ul>
<li><p>本模型主要用于分类。一些介绍</p>
<ul>
<li><p>我们进行数字分类最后输出就是<code>0-9</code>的概率，根据输出的概率选择最优的输出结果。</p>
</li>
<li><p>分类数据集比较基础的有<code>Mnist</code>、<code>CIFAR-10</code>等。</p>
</li>
<li><p>我们线性问题输出的是一个实数，我们要将所有的输出结果归一化再取概率，当然，这就出现了<code>sigmoid</code>函数。</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230614120612207.png" alt="image-20230614120612207"></p>
</li>
<li><p>上图对应的罗蒂函数：</p>
</li>
<li><p>$$<br>f\left( x \right) &#x3D;\frac{1}{1+e^{-x}}<br>$$</p>
</li>
<li><p>该函数当大于一个数或者小于一个数之后，导数值越来越小甚至趋于0，这种函数称为饱和函数。</p>
</li>
<li><p>一些其他的函数</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230614121311958.png" alt="image-20230614121311958"></p>
</li>
<li></li>
</ul>
</li>
<li><p>采取非线性模型，比如<code>sigmoid</code>会使得模型不再是线性模型，同时输出结果也不是一维输出，之前的线性模型是：</p>
<ul>
<li><p>$$<br>loss&#x3D;\left( \hat{y}-y \right) ^2&#x3D;\left( x\cdot \omega -y \right) ^2<br>$$</p>
</li>
<li><p>我们可以理解这是一个在坐标轴求两个点距离，这样的话距离越接近目标位置越好。</p>
</li>
<li><p>当我们的模型变成非线性的时候，计算损失函数(交叉熵)：</p>
</li>
<li><p>$$<br>loss&#x3D;-\left( y\log \hat{y}+\left( 1-y \right) \log \left( 1-\hat{y} \right) \right)<br>$$</p>
</li>
<li><p>这就是求熵来计算损失，此时输出结果是满足一个分布的，即：</p>
</li>
<li><p>$$<br>f\left( x \right) &#x3D;\frac{1}{1+e^{-x}}<br>$$</p>
</li>
<li><p>相当于计算分布的差异值。</p>
</li>
</ul>
</li>
<li><p>损失求解</p>
<ul>
<li><p>上面提到单个损失求解办法：</p>
</li>
<li><p>$$<br>loss&#x3D;-\left( y\log \hat{y}+\left( 1-y \right) \log \left( 1-\hat{y} \right) \right)<br>$$</p>
</li>
<li><p>我们继续求解小批量的损失：</p>
</li>
<li><p>$$<br>loss&#x3D;-\frac{1}{N}\sum_{n&#x3D;1}^N{\left( y_n\log \hat{y}_n+\left( 1-y_n \right) \log \left( 1-\hat{y}_n \right) \right)}<br>$$</p>
</li>
<li><p>上述<code>N</code>为小批量的大小。</p>
</li>
<li><p>在代码由<code>MSELoss</code>变为<code>BCELoss</code></p>
</li>
<li><p>对于代码<code>torch.nn.BCELoss(size_average=False)</code>其实为<code>True</code>的话，我们求的导数将会加上<code>1/N</code>不求的话没有。</p>
</li>
</ul>
</li>
<li><p>代码：</p>
<ul>
<li><pre><code class="python">%matplotlib inline
import torch
from torch import nn
from torch.nn import functional as F
import MyPrint as mp
import torchvision

# 设置设备
device = torch.device(&#39;cuda:0&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;)

# 加载数据
x_data = torch.tensor([1.0, 2.0, 3.0], device=device).reshape(3, 1)
y_data = torch.tensor([0, 0, 1], dtype=torch.float32, device=device).reshape(3, 1)

# 构建模型
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(1, 1)

    def forward(self, x):
        x = self.fc1(x)
        return F.sigmoid(x)

net = Net()
net = net.to(device)

# 损失函数
loss = nn.BCELoss(reduction=&#39;sum&#39;)

# 模型训练
epochs, lr = 1000, 1e-0
optimizer = torch.optim.SGD(net.parameters(), lr=lr)

epoch_list = []
loss_list = []
for epoch in range(epochs):
    y_hat = net(x_data)
    loss_num = loss(y_hat, y_data)
    loss_list.append(loss_num.item())
    epoch_list.append(epoch + 1)
    print(f&#39;epoch = &#123;epoch + 1&#125;, loss_num = &#123;loss_num.item()&#125;&#39;)
    optimizer.zero_grad()
    loss_num.backward()
    optimizer.step()
    
animator = mp.Animator(xlabel=&#39;epoch&#39;, ylabel=&#39;loss&#39;, legend=[&#39;train_loss&#39;], xlim=[1, epochs],
                       ylim=[0, 6])
animator.add(epoch_list, loss_list, is_change=False)

# 预测
x_test = torch.tensor([a for a in range(100)], dtype=torch.float32, device=device).reshape(-1, 1)
y_test = net(x_test)
x_pr = [x0.item() for x0 in x_test]
y_pr = [y0.item() for y0 in y_test]
animator2 = mp.Animator(xlabel=&#39;x&#39;, ylabel=&#39;test&#39;, legend=[&#39;test&#39;], xlim=[0, 11])
animator2.add(x_pr, y_pr, is_change=False)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  - 训练的图如下：</span><br><span class="line"></span><br><span class="line">  - ![image-20230614141023783](../pictures/pytorch快速入门/image-20230614141023783.png)</span><br><span class="line"></span><br><span class="line">  - 预测图如下：</span><br><span class="line"></span><br><span class="line">  - ![image-20230614141038988](../pictures/pytorch快速入门/image-20230614141038988.png)</span><br><span class="line"></span><br><span class="line">  - 可以看出，当我们数字大于3时基本上全是1，小于3的为0。</span><br><span class="line"></span><br><span class="line">## 7 处理多维特征的输入</span><br><span class="line"></span><br><span class="line">- 高维数据如下：</span><br><span class="line"></span><br><span class="line">  - ![image-20230614141408539](../pictures/pytorch快速入门/image-20230614141408539.png)</span><br><span class="line">  - 这个数据从行看过去，每一行就是一个样本，从列看过去，每一列就是一个特征值。</span><br><span class="line">  - 上图数据相当于一个样本有8个特征值。</span><br><span class="line"></span><br><span class="line">- 多维数据，模型1也发生改变：</span><br><span class="line"></span><br><span class="line">  - 模型如下：</span><br><span class="line"></span><br><span class="line">  - $$</span><br><span class="line">    \hat&#123;y&#125;_i=\sigma \left( x^&#123;\left( i \right)&#125;\cdot x+b \right) </span><br><span class="line">    $$</span><br><span class="line"></span><br><span class="line">  - 改变的模型如下：</span><br><span class="line"></span><br><span class="line">  - $$</span><br><span class="line">    \hat&#123;y&#125;_i=\sigma \left( \sum_&#123;n=1&#125;^8&#123;x^&#123;\left( i \right)&#125;\cdot x&#125;+b \right) </span><br><span class="line">    $$</span><br><span class="line"></span><br><span class="line">  - 上式中`i`表示第`i`个标量的意思。</span><br><span class="line"></span><br><span class="line">- 本节用的模型如下：</span><br><span class="line"></span><br><span class="line">  - ![image-20230614145812942](../pictures/pytorch快速入门/image-20230614145812942.png)</span><br><span class="line"></span><br><span class="line">  - 用上述模型来训练上面的数据。</span><br><span class="line"></span><br><span class="line">  - 代码如下：</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    %matplotlib inline</span><br><span class="line">    import torch</span><br><span class="line">    from torch import nn</span><br><span class="line">    from torch.nn import functional as F</span><br><span class="line">    import MyPrint as mp</span><br><span class="line">    import numpy as np</span><br><span class="line">    </span><br><span class="line">    # 设置设备</span><br><span class="line">    device = torch.device(&#x27;cuda:0&#x27;)</span><br><span class="line">    </span><br><span class="line">    # 加载数据</span><br><span class="line">    xy = np.loadtxt(&#x27;diabetes.csv.gz&#x27;, delimiter=&#x27;,&#x27;, dtype=np.float32)</span><br><span class="line">    x_data = torch.from_numpy(xy[:, :-1])</span><br><span class="line">    y_data = torch.from_numpy(xy[:, -1]).reshape(-1, 1)</span><br><span class="line">    x_data = x_data.to(device)</span><br><span class="line">    y_data = y_data.to(device)</span><br><span class="line">    </span><br><span class="line">    # 构建模型</span><br><span class="line">    class Net(nn.Module):</span><br><span class="line">        def __init__(self):</span><br><span class="line">            super(Net, self).__init__()</span><br><span class="line">            self.model = nn.Sequential(nn.Linear(8, 18),</span><br><span class="line">                                       nn.Sigmoid(),</span><br><span class="line">                                       nn.Linear(18, 9),</span><br><span class="line">                                       nn.Sigmoid(),</span><br><span class="line">                                       nn.Linear(9, 2),</span><br><span class="line">                                       nn.Sigmoid(),</span><br><span class="line">                                       nn.Linear(2, 1),</span><br><span class="line">                                       nn.Sigmoid())</span><br><span class="line">    </span><br><span class="line">        def forward(self, x):</span><br><span class="line">            x = self.model(x)</span><br><span class="line">            return x</span><br><span class="line">    </span><br><span class="line">    net = Net()</span><br><span class="line">    net = net.to(device=device)</span><br><span class="line">    </span><br><span class="line">    # 选取损失函数</span><br><span class="line">    loss = nn.BCELoss(reduction=&#x27;sum&#x27;)</span><br><span class="line">    </span><br><span class="line">    # 开始训练</span><br><span class="line">    epochs, lr = 100, 1e-3</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    </span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        y_hat = net(x_data)</span><br><span class="line">        loss_num = loss(y_hat, y_data)</span><br><span class="line">        print(loss_num.item())</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss_num.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="8-加载数据集"><a href="#8-加载数据集" class="headerlink" title="8 加载数据集"></a>8 加载数据集</h2><p>数据集的处理可以使用随机梯度下降，得到的目标效果较好。不进行数据分类，我们在目标结果的查找可能较差。</p>
<ul>
<li><p>一些概念：</p>
<ul>
<li>epoch：所有的样本经历一次前向传播和反向传播为一次epoch。</li>
<li>Batch_size：我们每次训练所用的样本数量。</li>
<li>Iteration：内层迭代的次数。</li>
</ul>
</li>
<li><p>数据集(Dataset)和DataLoader</p>
<ul>
<li><p>我们有一个数据集如下：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230614180246492.png" alt="image-20230614180246492"></p>
</li>
<li><p>我们设置<code>shuffle=True</code>，这个参数就是随机打乱顺序：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230614180446116.png" alt="image-20230614180446116"></p>
</li>
<li><p>再利用<code>batch_size=2</code>进行分组，也就是两个分一组：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230614180531327.png" alt="image-20230614180531327"></p>
</li>
<li><p>得到的DataLoader可以用于每次数据迭代，每次迭代都会给一个Batch。</p>
</li>
</ul>
</li>
<li><p>代码实现Dataset和DataLoader</p>
<ul>
<li><p>Dataset时一个抽象类，我们可以自己定义一个类继承该类。</p>
</li>
<li><p>抽象类是不能实例化只能继承。</p>
</li>
<li><p>使用DataLoader产生的数据集，由于<code>num_workers</code>并行的原因，我们无法直接调用该数据：</p>
</li>
<li><pre><code class="python">for i, data in enumerate(train_loader, 0):
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 否则会报错，我们需要将其进行封装：</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  if __name__==&#x27;__main__&#x27;:</span><br><span class="line">  	for i, data in enumerate(train_loader, 0):</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>实际上自己还没遇到，这个教程只是来源于视频，待会自己去测试看看。</p>
</li>
<li><pre><code class="python">%matplotlib inline
import torch
from torch import nn
import MyPrint as mp
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

# 设备选择
device = torch.device(&#39;cuda:0&#39;)

# 定义自己的Dataset和DataLoader
class DiabetesDataset(Dataset):
    def __init__(self, filepath):
        # 读取数据
        xy = np.loadtxt(filepath, delimiter=&#39;,&#39;, dtype=np.float32)
        # 获取样本数
        self.len = xy.shape[0]
        self.x_data = torch.from_numpy(xy[:, :-1])
        self.y_data = torch.from_numpy(xy[:, [-1]])

    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    def __len__(self):
        return self.len

dataset = DiabetesDataset(&#39;diabetes.csv.gz&#39;)
# num_workers表示线程，等于2表示2线程
train_loader = DataLoader(dataset, batch_size=32, shuffle=True)

# 定义网络
# 构建模型
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.model = nn.Sequential(nn.Linear(8, 6),
                                   nn.ReLU(),
                                   nn.Linear(6, 4),
                                   nn.ReLU(),
                                   nn.Linear(4, 1),
                                   nn.Sigmoid())

    def forward(self, x):
        x = self.model(x)
        return x

net = Net()
net = net.to(device=device)

# 选取损失函数
loss = nn.BCELoss(reduction=&#39;sum&#39;)

# 开始训练
epochs, lr = 1000, 1e-3
optimizer = torch.optim.SGD(net.parameters(), lr=lr)
animator = mp.Animator(xlabel=&#39;epoch&#39;, ylabel=&#39;loss&#39;, legend=[&#39;train_loss&#39;])

for epoch in range(epochs):
    loss_sum = []
    loss_sum0 = 0
    for i, data in enumerate(train_loader, 0):
        features, labels = data
        features, labels = features.to(device), labels.to(device)
        y_hat = net(features)
        loss_num = loss(y_hat, labels)
        loss_sum0 += loss_num.item()
        optimizer.zero_grad()
        loss_num.backward()
        optimizer.step()
    loss_sum.append(loss_sum0 / len(dataset))
    animator.add(epoch + 1, loss_sum0 / len(dataset))
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- torchvision.datasets</span><br><span class="line"></span><br><span class="line">  - torchvision里面提供大量的数据集。</span><br><span class="line">  - `MNIST`就是一个手写数据集。</span><br><span class="line">  - ![image-20230614185538680](../pictures/pytorch快速入门/image-20230614185538680.png)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 9 多分类问题</span><br><span class="line"></span><br><span class="line">- 介绍</span><br><span class="line"></span><br><span class="line">  - 多分类问题会用到`softmax`分类器</span><br><span class="line"></span><br><span class="line">- 设计</span><br><span class="line"></span><br><span class="line">  - 我们假设有10个类，那我们就将输出设置为10个输出。每个输出都有一个概率，根据概率来选择属于哪一类。</span><br><span class="line"></span><br><span class="line">  - 所以模型设计如下：</span><br><span class="line"></span><br><span class="line">  - ![image-20230614191453827](../pictures/pytorch快速入门/image-20230614191453827.png)</span><br><span class="line"></span><br><span class="line">  - softmax的计算公式如下：</span><br><span class="line"></span><br><span class="line">  - $$</span><br><span class="line">    P\left( y=i \right) =\frac&#123;e^&#123;Z_i&#125;&#125;&#123;\sum\nolimits_&#123;j=0&#125;^&#123;K-1&#125;&#123;e^&#123;Z_j&#125;&#125;&#125;,i\in \left( 0,...,K-1 \right) </span><br><span class="line">    $$</span><br><span class="line"></span><br><span class="line">  - 之所以用指数，可以将负数值化为正数。保证输出的值都大于0，而且其总和也是1。</span><br><span class="line"></span><br><span class="line">  - 举个栗子：</span><br><span class="line"></span><br><span class="line">  - ![image-20230614192057802](../pictures/pytorch快速入门/image-20230614192057802.png)</span><br><span class="line"></span><br><span class="line">  - 数学真妙。</span><br><span class="line"></span><br><span class="line">- 损失函数的设计：</span><br><span class="line"></span><br><span class="line">  - 损失函数的设计如下：</span><br><span class="line"></span><br><span class="line">  - $$</span><br><span class="line">    Loss\left( \hat&#123;Y&#125;,Y \right) =-Y\log \hat&#123;Y&#125;</span><br><span class="line">    $$</span><br><span class="line"></span><br><span class="line">  - 封装的函数是：`NLLLoss`</span><br><span class="line"></span><br><span class="line">  - ![image-20230614192955233](../pictures/pytorch快速入门/image-20230614192955233.png)</span><br><span class="line"></span><br><span class="line">  - 整个过程也封装在`nn.CrossEntropyLoss`里面(交叉熵损失)。</span><br><span class="line"></span><br><span class="line">  - 使用交叉熵损失在网络的定义中不需要加上`softmax`。</span><br><span class="line"></span><br><span class="line">  - 但是labels的类型必须是`LongTensor`类型。</span><br><span class="line"></span><br><span class="line">  - 一个栗子：</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    import torch</span><br><span class="line">    from torch import nn</span><br><span class="line">    </span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    Y = torch.LongTensor([2, 0, 1])</span><br><span class="line">    Y_pred1 = torch.Tensor([[0.1, 0.2, 0.9],</span><br><span class="line">                            [1.1, 0.1, 0.2],</span><br><span class="line">                            [0.2, 2.1, 0.1]])</span><br><span class="line">    Y_pred2 = torch.Tensor([[0.8, 0.2, 0.3],</span><br><span class="line">                            [0.2, 0.3, 0.5],</span><br><span class="line">                            [0.2, 0.2, 0.5]])</span><br><span class="line">    l1 = criterion(Y_pred1, Y)</span><br><span class="line">    l2 = criterion(Y_pred2, Y)</span><br><span class="line">    print(f&#x27;Loss1 = &#123;l1&#125;, Loss2 = &#123;l2&#125;&#x27;)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>代码中<code>Y</code>是第几个输出结果应该对应的最大值位置。</p>
</li>
<li><p>上面位置应该从0开始索引。</p>
</li>
<li><p>通过上述也可以看出一个关系：</p>
</li>
<li><p><code>CrossEntropyLoss==LogSoftmax+NLLLoss</code></p>
</li>
</ul>
</li>
<li><p>现在用图像来讲解：</p>
<ul>
<li>我们以<code>MNIST</code>数据集图片作为讲解：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230614194537155.png" alt="image-20230614194537155"></li>
<li>上图来自于<code>MNIST</code>图像集；上图黑色部分的数值一般比较小，亮一点的部分数值比较大。一张图片的大小为<code>28*28=784</code>也就是784个像素。每个像素取值大小为0-255，我们将其映射到0-1区间可以得到下图：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230614194849957.png" alt="image-20230614194849957"></li>
</ul>
</li>
<li><p>实现<code>MNIST</code>数据集的分类问题。</p>
<ul>
<li><p><strong>加载数据：</strong></p>
</li>
<li><pre><code class="python"># 加载数据
batch_size = 64
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307, ), (0.3081, ))])
train_data = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=True, transform=transform, download=True)
test_data = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=False, transform=transform, download=True)
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- `ToTensor()`将`PIL`转为`Tensor`类型。</span><br><span class="line"></span><br><span class="line">- `transforms.Normalize`两个参数，前个是均值，后一个是标准差。将数值计算到0-1范围内。上述两个数值是整个`MNIST`图像的均值和标准差，这样可以将所有图片数值都限制在0-1之间。</span><br><span class="line"></span><br><span class="line">- `PIL`的值是0-255区间，我们需要将值转变为张量，我们再将该值转换为0-1区间的值。</span><br><span class="line"></span><br><span class="line">- 流程图如下：</span><br><span class="line"></span><br><span class="line">- ![image-20230614201037437](../pictures/pytorch快速入门/image-20230614201037437.png)</span><br><span class="line"></span><br><span class="line">- **建立模型：**</span><br><span class="line"></span><br><span class="line">- 我们输入的图片的参数是`(N,1,28,28)`我们需要将每一张图片看作一个样本，构建数据矩阵，特征值是784个。</span><br><span class="line"></span><br><span class="line">- ![image-20230614201922699](../pictures/pytorch快速入门/image-20230614201922699.png)</span><br><span class="line"></span><br><span class="line">- 整个模型：</span><br><span class="line"></span><br><span class="line">- ![image-20230614202019679](../pictures/pytorch快速入门/image-20230614202019679.png)</span><br><span class="line"></span><br><span class="line">- 代码如下：</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  # 构建模型</span><br><span class="line">  class Net(nn.Module):</span><br><span class="line">      def __init__(self):</span><br><span class="line">          super(Net, self).__init__()</span><br><span class="line">          self.model = nn.Sequential(nn.Linear(784, 512),</span><br><span class="line">                                     nn.ReLU(),</span><br><span class="line">                                     nn.Linear(512, 256),</span><br><span class="line">                                     nn.ReLU(),</span><br><span class="line">                                     nn.Linear(256, 128),</span><br><span class="line">                                     nn.ReLU(),</span><br><span class="line">                                     nn.Linear(128, 64),</span><br><span class="line">                                     nn.ReLU(),</span><br><span class="line">                                     nn.Linear(64, 10))</span><br><span class="line">  </span><br><span class="line">      def forward(self, x):</span><br><span class="line">          x = x.view(-1, 784)</span><br><span class="line">          x = self.model(x)</span><br><span class="line">          return x</span><br><span class="line">  </span><br><span class="line">  net = Net()</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>总的代码：</p>
</li>
<li><pre><code class="python">import torch
from torch import nn
import torchvision
from torch.utils.data import DataLoader
from torchvision import transforms
import MyPrint as mp

# 设备
device = torch.device(&#39;cuda:0&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;)

# 加载数据
batch_size = 64
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307, ), (0.3081, ))])
train_data = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=True, transform=transform, download=True)
test_data = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=False, transform=transform, download=True)
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

# 构建模型
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.model = nn.Sequential(nn.Linear(784, 512),
                                   nn.ReLU(),
                                   nn.Linear(512, 256),
                                   nn.ReLU(),
                                   nn.Linear(256, 128),
                                   nn.ReLU(),
                                   nn.Linear(128, 64),
                                   nn.ReLU(),
                                   nn.Linear(64, 10))

    def forward(self, x):
        x = x.reshape(-1, 784)
        x = self.model(x)
        return x

net = Net()
net = net.to(device)

# 得到损失函数
loss = nn.CrossEntropyLoss()

# 开始训练
epochs, lr = 10, 1e-2
# momentum=0.5
optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.5)
animator = mp.Animator(xlabel=&#39;epoch&#39;, ylabel=&#39;loss&#39;, legend=[&#39;train_loss&#39;, &#39;test_loss&#39;])

for epoch in range(epochs):
    train_loss = 0
    test_loss = 0
    for i, data in enumerate(train_loader, 0):
        features, labels = data
        features, labels = features.to(device), labels.to(device)
        y_hat = net(features)
        loss_num = loss(y_hat, labels)
        train_loss += loss_num.item()
        optimizer.zero_grad()
        loss_num.backward()
        optimizer.step()
    # 测试
    with torch.no_grad():
        for i, data in enumerate(test_loader, 0):
            features, labels = data
            features, labels = features.to(device), labels.to(device)
            y_hat = net(features)
            loss_num = loss(y_hat, labels)
            test_loss += loss_num.item()
    # 绘图
    animator.add(epoch + 1, (train_loss, test_loss))
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 单独计算正确率：</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  # 开始训练</span><br><span class="line">  epochs, lr = 10, 1e-2</span><br><span class="line">  # momentum=0.5</span><br><span class="line">  optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.5)</span><br><span class="line">  animator = mp.Animator(xlabel=&#x27;epoch&#x27;, ylabel=&#x27;loss&#x27;, legend=[&#x27;train_loss&#x27;, &#x27;test_loss&#x27;])</span><br><span class="line">  </span><br><span class="line">  for epoch in range(epochs):</span><br><span class="line">      train_loss = 0</span><br><span class="line">      correct = 0</span><br><span class="line">      total = 0</span><br><span class="line">      for i, data in enumerate(train_loader, 0):</span><br><span class="line">          features, labels = data</span><br><span class="line">          features, labels = features.to(device), labels.to(device)</span><br><span class="line">          y_hat = net(features)</span><br><span class="line">          loss_num = loss(y_hat, labels)</span><br><span class="line">          train_loss += loss_num.item()</span><br><span class="line">          optimizer.zero_grad()</span><br><span class="line">          loss_num.backward()</span><br><span class="line">          optimizer.step()</span><br><span class="line">      # 测试</span><br><span class="line">      with torch.no_grad():</span><br><span class="line">          for i, data in enumerate(test_loader, 0):</span><br><span class="line">              features, labels = data</span><br><span class="line">              features, labels = features.to(device), labels.to(device)</span><br><span class="line">              y_hat = net(features)</span><br><span class="line">              _, predict_index = torch.max(y_hat.data, dim=1)</span><br><span class="line">              total += labels.shape[0]</span><br><span class="line">              correct += (predict_index == labels).sum().item()</span><br><span class="line">      # 绘图</span><br><span class="line">      animator.add(epoch + 1, correct / total)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p>图像的一点补充：</p>
<ul>
<li>我们平常见到的黑白图像是单通道图像，彩色图片一般有三通道，如下：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230614200737807.png" alt="image-20230614200737807"></li>
<li>一个单通道图像有宽和高，多通道图像还有管道数(channel)用<code>c</code>表示。上文中我们用<code>PIL</code>读取进入的数值为<code>WxHxC</code>。在<code>pytorch</code>里面需要将其转换为<code>CxWxH</code>也就是将通道数放在最前面。</li>
</ul>
</li>
</ul>
<h2 id="10-卷积神经网络-基础篇"><a href="#10-卷积神经网络-基础篇" class="headerlink" title="10 卷积神经网络(基础篇)"></a>10 卷积神经网络(基础篇)</h2><ul>
<li><p>介绍</p>
<ul>
<li>处理图像经常用二维卷积网络。</li>
<li>卷积层要保留图像的空间特征。</li>
<li>下采样可以将像素减小，通道数不变，也就是减少数据量降低运算需求。</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230614210809123.png" alt="image-20230614210809123"></li>
<li>上图中的卷积层叫做特征提取器(Feature Extraction)。</li>
</ul>
</li>
<li><p>图像再介绍</p>
<ul>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230614211119038.png" alt="image-20230614211119038"></li>
<li>上面是一张图</li>
<li>我们计算机用的图片一般都是<code>RGB</code>格式的图像，也就是栅格图像。</li>
<li>我们获取图像都是通过ccd来获取。</li>
<li><code>ccd</code>的原理和光敏电阻有关，如下一个电路图：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615123046964.png" alt="image-20230615123046964"></li>
<li>根据电路的电流值与电压值可以求出光敏电阻的阻值，进而得到外界的光强大小。</li>
<li>现在假设有一个透镜在前面，光线经过透镜打到光敏电阻上，此时该光敏电阻只检测很小的锥形空间：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615123449054.png" alt="image-20230615123449054"></li>
<li>如果我们做一堆的电阻如下：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615123554489.png" alt="image-20230615123554489"></li>
<li>这样每个光敏电阻可以叫像素，假设我们有4个电阻排列，经过得到阻值根据一定的函数关系转变为光的强度：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615123827098.png" alt="image-20230615123827098"></li>
<li>那么此时的像素就是<code>2*2=4</code>也就是4个像素。这里假设有800行×600行的矩阵，我们就得到48w的像素。</li>
<li>这样是黑白图像，对于彩色图像我们还需要改进：</li>
<li>比如布局传感器处理像素如下：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615124341141.png" alt="image-20230615124341141"></li>
<li>每个传感器的光敏电阻不同，颜色不同强度不同，得到的映射结果也不一样。每个颜色的传感器对不同颜色的敏感度不一样。</li>
<li>这样我们如果是<code>RGB</code>图片的话，我们的输入就有三个<code>Channel</code>:</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615130518567.png" alt="image-20230615130518567"></li>
<li>我们在深度学习需要取上述的<code>Patch</code>块，它的大小就是：<code>3*W*H</code>的张量。再把图像块拿出来做卷积。</li>
<li>图片的原点在左上角。</li>
</ul>
</li>
<li><p>卷积的运算过程</p>
<ul>
<li><strong>单通道卷积</strong></li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615130951695.png" alt="image-20230615130951695"></li>
<li><strong>多通道的卷积</strong></li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615131128117.png" alt="image-20230615131128117"></li>
<li>上述的3*3红色框就是一个<code>Patch</code>值。</li>
<li>卷积核类似权重，在训练过程会自动更新。</li>
<li>上述过程简化：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615131308701.png" alt="image-20230615131308701"></li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615131340866.png" alt="image-20230615131340866"></li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615131411647.png" alt="image-20230615131411647"></li>
<li><strong>多输出通道：</strong></li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615131559511.png" alt="image-20230615131559511"></li>
<li>上述第二层可以理解为一个4维的矩阵。</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615131829079.png" alt="image-20230615131829079"></li>
<li><strong>总结</strong></li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615132008265.png" alt="image-20230615132008265"></li>
<li>所以我们构建卷积层，它的维度就是这种维度(上图红框)</li>
</ul>
</li>
<li><p>卷积层的代码实现</p>
<ul>
<li><pre><code class="python">import torch
from torch import nn

in_channels, out_channels = 5, 10
width, height = 100, 100
kernel_size = 3
batch_size = 1

input_num = torch.randn(batch_size, in_channels, width, height)
conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size)
output_num = conv_layer(input_num)
print(input_num.shape) # 1x5x100x100
print(output_num.shape) # 10xWxH
print(conv_layer.weight.shape) # 10x5x3x3
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 卷积层的其它参数</span><br><span class="line"></span><br><span class="line">  - **`padding`**</span><br><span class="line"></span><br><span class="line">  - padding默认填充0，填充可以根据`kernel_size/2`进行选择，使得输入与输出一样大。</span><br><span class="line"></span><br><span class="line">  - ![image-20230615133537002](../pictures/pytorch快速入门/image-20230615133537002.png)</span><br><span class="line"></span><br><span class="line">  - `padding=1`与`padding=(1, 1)`一样，都是上下左右各加一层。</span><br><span class="line"></span><br><span class="line">  - 代码如下：</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    import torch</span><br><span class="line">    from torch import nn</span><br><span class="line">    </span><br><span class="line">    input_num = [3, 4, 6, 5, 7,</span><br><span class="line">                 2, 4, 6, 8, 2,</span><br><span class="line">                 1, 6, 7, 8, 4,</span><br><span class="line">                 9, 7, 4, 6, 2,</span><br><span class="line">                 3, 7, 5, 4, 1]</span><br><span class="line">    input_num = torch.tensor(input_num, dtype=torch.float32).reshape(1, 1, 5, 5)</span><br><span class="line">    conv_layer = nn.Conv2d(1, 1, 3, padding=(1, 1), bias=False)</span><br><span class="line">    kernel = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]).reshape(1, 1, 3, 3)</span><br><span class="line">    conv_layer.weight.data = kernel.data</span><br><span class="line">    output_num = conv_layer(input_num)</span><br><span class="line">    print(output_num)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p><strong><code>stride</code></strong></p>
</li>
<li><p>这是步长的意思，默认为1，步长为2的话每次索引下一个<code>Patch</code>都会加2。</p>
</li>
<li><p>代码如下：</p>
</li>
<li><pre><code class="python">input_num = [3, 4, 6, 5, 7,
             2, 4, 6, 8, 2,
             1, 6, 7, 8, 4,
             9, 7, 4, 6, 2,
             3, 7, 5, 4, 1]
input_num = torch.tensor(input_num, dtype=torch.float32).reshape(1, 1, 5, 5)
conv_layer = nn.Conv2d(1, 1, 3, stride=2, bias=False)
kernel = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]).reshape(1, 1, 3, 3)
conv_layer.weight.data = kernel.data
output_num = conv_layer(input_num)
print(output_num)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- **下采样**</span><br><span class="line"></span><br><span class="line">- 常用的下采样是`MaxPooling`，叫最大池化层，它是没有权重的。比如有下面的数据：</span><br><span class="line"></span><br><span class="line">- ![image-20230615134639642](../pictures/pytorch快速入门/image-20230615134639642.png)</span><br><span class="line"></span><br><span class="line">- 我们对其做`maxpooling`，它会以4个值为一个范围选取最大值得到结果如下：</span><br><span class="line"></span><br><span class="line">- ![image-20230615134733631](../pictures/pytorch快速入门/image-20230615134733631.png)</span><br><span class="line"></span><br><span class="line">- 做下采样通道数是不变的。</span><br><span class="line"></span><br><span class="line">- 代码如下：</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  import torch</span><br><span class="line">  from torch import nn</span><br><span class="line">  </span><br><span class="line">  input_num = [3, 4, 6, 5,</span><br><span class="line">               2, 4, 6, 8,</span><br><span class="line">               1, 6, 7, 8,</span><br><span class="line">               9, 7, 4, 6]</span><br><span class="line">  input_num = torch.tensor(input_num, dtype=torch.float32).reshape(1, 1, 4, 4)</span><br><span class="line">  max_layer = nn.MaxPool2d(kernel_size=2)</span><br><span class="line">  output_num = max_layer(input_num)</span><br><span class="line">  print(output_num)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p><code>kernel_size=2</code>与<code>kernel_size=(2, 2)</code>一致。</p>
</li>
</ul>
</li>
<li><p>实现简单的神经网络</p>
<ul>
<li><p>网络模型</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615135822648.png" alt="image-20230615135822648"></p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615135907023.png" alt="image-20230615135907023"></p>
</li>
<li><p>使用<code>MNIST</code>数据集，代码如下：</p>
</li>
<li><pre><code class="python">import torch
from torch import nn
from torch.utils.data import DataLoader
import torchvision
from torchvision import transforms
import MyPrint as mp

# 定义设备
device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

# 加载数据
batch_size = 32
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307, ), (0.3081, ))])
train_data = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=True, transform=transform, download=True)
test_data = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=False, transform=transform, download=True)
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

# 定义模型
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.model = nn.Sequential(nn.Conv2d(1, 10, kernel_size=5),
                                   nn.ReLU(),
                                   nn.MaxPool2d(kernel_size=2),
                                   nn.Conv2d(10, 20, kernel_size=5),
                                   nn.ReLU(),
                                   nn.MaxPool2d(kernel_size=2),
                                   nn.Flatten(),
                                   nn.Linear(320, 240),
                                   nn.ReLU(),
                                   nn.Linear(240, 120),
                                   nn.ReLU(),
                                   nn.Linear(120, 40),
                                   nn.ReLU(),
                                   nn.Linear(40, 10))
    def forward(self, x):
        x = self.model(x)
        return x

net = Net()
net = net.to(device)

# 构建损失函数
loss = nn.CrossEntropyLoss()

# 开始训练
epochs, lr = 20, 1e-2
optimizer = torch.optim.SGD(net.parameters(), lr=lr)
animator = mp.Animator(xlabel=&#39;epoch&#39;, ylabel=&#39;loss&#39;, legend=[&#39;test&#39;])

for epoch in range(epochs):
    for i, data in enumerate(train_loader, 0):
        features, labels = data
        features, labels = features.to(device), labels.to(device)
        y_hat = net(features)
        loss_num = loss(y_hat, labels)
        optimizer.zero_grad()
        loss_num.backward()
        optimizer.step()
    # 测试
    with torch.no_grad():
        correct = 0
        total = 0
        for i, data in enumerate(test_loader, 0):
            features, labels = data
            features, labels = features.to(device), labels.to(device)
            y_hat = net(features)
            _, y_hat_index = torch.max(y_hat, dim=1)
            total += labels.shape[0]
            correct += (labels == y_hat_index).sum().item()
        animator.add(epoch + 1, correct / total)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">## 11 卷积神经网络(高级篇)</span><br><span class="line"></span><br><span class="line">- **GoogLeNet**</span><br><span class="line"></span><br><span class="line">  - 结构如下：</span><br><span class="line"></span><br><span class="line">  - ![image-20230615144330234](../pictures/pytorch快速入门/image-20230615144330234.png)</span><br><span class="line"></span><br><span class="line">  - `Other`表示拼接层。</span><br><span class="line"></span><br><span class="line">  - 可以看到上述代码有很多的重复结构：</span><br><span class="line"></span><br><span class="line">  - ![image-20230615144527693](../pictures/pytorch快速入门/image-20230615144527693.png)</span><br><span class="line"></span><br><span class="line">  - 我们可以单独定义出该结构，再串起来可以减小代码的冗余。</span><br><span class="line"></span><br><span class="line">  - 我们在python中常用函数或者类来减小代码冗余，在这里也类似。</span><br><span class="line"></span><br><span class="line">  - 上述红色圈就是`Inception Module`。下载单独介绍该模块。</span><br><span class="line"></span><br><span class="line">  - **`Inception Module`**</span><br><span class="line"></span><br><span class="line">  - 它的结构如下：</span><br><span class="line"></span><br><span class="line">  - ![image-20230615144853585](../pictures/pytorch快速入门/image-20230615144853585-1686811733817-1.png)</span><br><span class="line"></span><br><span class="line">  - 我都演我都要我都要！！！！！！！</span><br><span class="line"></span><br><span class="line">  - 最后的结果拼接需要保证宽度和高度是一致的。</span><br><span class="line"></span><br><span class="line">  - 我们数据的尺寸是`(b, c, w, h)`我们唯一可以变得是`c`，不然无法拼接。</span><br><span class="line"></span><br><span class="line">  - 这里介绍以下`1x1`卷积，我们通常可以用它来减小计算量：</span><br><span class="line"></span><br><span class="line">  - ![image-20230615150250008](../pictures/pytorch快速入门/image-20230615150250008.png)</span><br><span class="line"></span><br><span class="line">  - `1x1`的网络也叫网络的网络</span><br><span class="line"></span><br><span class="line">  - 代码实现`Inception Module`</span><br><span class="line"></span><br><span class="line">  - 我们实现的基本原理如下：</span><br><span class="line"></span><br><span class="line">  - ![image-20230615152410166](../pictures/pytorch快速入门/image-20230615152410166.png)</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    import torch</span><br><span class="line">    from torch import nn</span><br><span class="line">    </span><br><span class="line">    # 一些初始化</span><br><span class="line">    in_channels = 100</span><br><span class="line">    </span><br><span class="line">    class InceptionModel(nn.Module):</span><br><span class="line">        def __init__(self， in_channels):</span><br><span class="line">            super(InceptionModel, self).__init__()</span><br><span class="line">            self.model1 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1, padding=1),</span><br><span class="line">                                        nn.Conv2d(in_channels=in_channels, out_channels=24, kernel_size=1))</span><br><span class="line">            self.model2 = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=1))</span><br><span class="line">            self.model3 = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=1),</span><br><span class="line">                                        nn.Conv2d(in_channels=16, out_channels=24, kernel_size=5, padding=2))</span><br><span class="line">            self.model4 = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=1),</span><br><span class="line">                                        nn.Conv2d(in_channels=16, out_channels=24, kernel_size=3, padding=1),</span><br><span class="line">    </span><br><span class="line">                                        nn.Conv2d(in_channels=24, out_channels=24, kernel_size=3, padding=1))</span><br><span class="line">        def forward(self, x):</span><br><span class="line">            x1 = self.model1(x)</span><br><span class="line">            x2 = self.model2(x)</span><br><span class="line">            x3 = self.model3(x)</span><br><span class="line">            x4 = self.model4(x)</span><br><span class="line">            x = torch.cat([x1, x2, x3, x4], dim=1)</span><br><span class="line">            return x</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p><code>dim=1</code>也就是按照<code>(B, C, W, H)</code>的<code>C</code>进行拼接。</p>
</li>
</ul>
</li>
<li><p>梯度消失是因为一个小于1的数字一直乘，导致参数权重不再更新，梯度也是很接近零。解决方法之一就是逐层训练，训练好的层将其锁住。<code>ResNet</code>就是一种解决办法。</p>
</li>
<li><p><strong><code>ResNet</code></strong></p>
<ul>
<li><p>基本如下：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615154448886-1686815089244-3.png" alt="image-20230615154448886"></p>
</li>
<li><p>相加再激活</p>
</li>
<li><p>在加法这里模型的输出<code>F(x)</code>应该和<code>x</code>的宽度高度维度都得一样。</p>
</li>
<li><p>代码得实现：</p>
</li>
<li><pre><code class="python">class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.model = nn.Sequential(nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, padding=1),
                                   nn.ReLU(),
                                   nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, padding=1))

    def forward(self, x):
        y = self.model(x)
        return F.relu(x + y)

net = ResidualBlock(10)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 我们现在实现下面得网络：</span><br><span class="line"></span><br><span class="line">- ![image-20230615160107829](../pictures/pytorch快速入门/image-20230615160107829.png)</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  class ResidualBlock(nn.Module):</span><br><span class="line">      def __init__(self, channels):</span><br><span class="line">          super(ResidualBlock, self).__init__()</span><br><span class="line">          self.model = nn.Sequential(nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, padding=1),</span><br><span class="line">                                     nn.ReLU(),</span><br><span class="line">                                     nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, padding=1))</span><br><span class="line">  </span><br><span class="line">      def forward(self, x):</span><br><span class="line">          y = self.model(x)</span><br><span class="line">          return F.relu(x + y)</span><br><span class="line">  </span><br><span class="line">  class Net(nn.Module):</span><br><span class="line">      def __init__(self):</span><br><span class="line">          super(Net, self).__init__()</span><br><span class="line">          self.model = nn.Sequential(nn.Conv2d(1, 16, kernel_size=5),</span><br><span class="line">                                     nn.ReLU(),</span><br><span class="line">                                     nn.MaxPool2d(kernel_size=2),</span><br><span class="line">                                     ResidualBlock(16),</span><br><span class="line">                                     nn.Conv2d(16, 32, kernel_size=5),</span><br><span class="line">                                     nn.ReLU(),</span><br><span class="line">                                     nn.MaxPool2d(kernel_size=2),</span><br><span class="line">                                     ResidualBlock(32),</span><br><span class="line">                                     nn.Flatten(),</span><br><span class="line">                                     nn.Linear(512, 10))</span><br><span class="line">  </span><br><span class="line">      def forward(self, x):</span><br><span class="line">          x = self.model(x)</span><br><span class="line">          return x</span><br><span class="line">  </span><br><span class="line">  net = Net()</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
<li><p>接下来一点意见</p>
<ul>
<li>从理论的角度学习深度学习—深度学习得花书</li>
<li>阅读Pytorch文档</li>
<li>复现一些经典工作(复现论文)</li>
<li>选一个特定的领域大量阅读论文，写论文，扩充视野。不断解决知识上得盲点。</li>
</ul>
</li>
</ul>
<h2 id="12-循环神经网络-基础篇"><a href="#12-循环神经网络-基础篇" class="headerlink" title="12 循环神经网络(基础篇)"></a>12 循环神经网络(基础篇)</h2><ul>
<li><p>对于下面全连接网络我们也叫稠密网络：<code>Dense</code>或者也叫<code>Deep</code>。</p>
<ul>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615162734298.png" alt="image-20230615162734298"></li>
<li>输入数据就是<code>x1-x8</code>是数据得不同特征。</li>
<li>现在我们用这个数据来学习一点新内容：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615162959690.png" alt="image-20230615162959690"></li>
<li>上面得数据是一个小时采样一次，里面有温度、气压、下雨等数据。</li>
<li>如果我们在上表中用当前一个时刻得温度和气压去预测当前是否下雨是没有用的。</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615163135122.png" alt="image-20230615163135122"></li>
<li>我们应该拿之前得许多数据进行训练，也就是之前得时间得数据。</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615163301291.png" alt="image-20230615163301291"></li>
<li>但是如果我们数据很长，每天得数据特征值很多，那么网络的权重会很多。</li>
<li>我们现在研究的问题是各种数据都存在依赖关系：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615163914837.png" alt="image-20230615163914837"></li>
<li>天气的变化就是一种比较依赖之前的变化。</li>
<li>天气、股市、自然语言都是有依赖的。</li>
<li>自然语言如：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615164032376.png" alt="image-20230615164032376"></li>
<li>这就是一个序列关系，它是有先后顺序的。</li>
<li>我们使用<code>RNN</code>处理这种序列相关的问题。</li>
</ul>
</li>
<li><p><code>RNN</code>介绍</p>
<ul>
<li>如下图：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615164220681.png" alt="image-20230615164220681"></li>
<li>假设我们输入的是三维的变量，输出是一个5维的变量，那么我们很容易知道<code>RNN</code>是一个线性层。</li>
<li><code>RNN</code>的线性层是共享的：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615164344791.png" alt="image-20230615164344791"></li>
<li>输出<code>h</code>叫<code>hidden</code>。我们假设以<code>x2</code>的输出<code>h2</code>来说明，我们为了得到<code>h2</code>的结果，我们输出不仅需要<code>x2</code>的信息，还需要<code>x1</code>的信息；这里的融合可以看作，当我们输出<code>h1</code>后，我们还将<code>h1</code>送到<code>h2</code>的计算过程中。</li>
<li>我们如果以天气为例，上面的输入就是一些特征，如温度，压强等。</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615164626512.png" alt="image-20230615164626512"></li>
<li><code>h0</code>作为先验。如果没有先验证，我们将<code>h0</code>设置为0，但是维度要和<code>h2</code>以及<code>h1</code>等一样。</li>
</ul>
</li>
<li><p>**<code>RNN</code>**计算过程</p>
<ul>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615175739384.png" alt="image-20230615175739384"></p>
</li>
<li><p>我们输入的<code>xt</code>维度是<code>input_size x 1</code>所以上面的<code>Wih</code>是<code>hidden_size x input_size</code>得到的输出结果是<code>hidden_size x 1</code>，两个相加可以实现信息的融合。融合后进行激活，循环神经网络通常用<code>tanh</code>做激活函数。算出的激活为结果。</p>
</li>
<li><p>pytorch实现代码如下：</p>
</li>
<li><pre><code class="python">nn.RNNCell(input_size=1, hidden_size=1)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 代码中需要自己实现`input_size`和`hidden_size`参数的传入。</span><br><span class="line"></span><br><span class="line">- 我们这样实现需要借助循环：</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  cell = nn.RNNCell(input_size=1, hidden_size=1)</span><br><span class="line">  # 循环</span><br><span class="line">  hidden = cell(input, hidden)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>输入的维度和隐藏的维度以及输出的维度如下图所示：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615182131904.png" alt="image-20230615182131904"></p>
</li>
<li><p>举个栗子：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615182218771.png" alt="image-20230615182218771"></p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615182251982.png" alt="image-20230615182251982"></p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615182307241.png" alt="image-20230615182307241"></p>
</li>
<li><p>代码如下：</p>
</li>
<li><pre><code class="python">import torch
from torch import nn

# 设置参数
batch_size = 1
seq_len = 3
input_size = 4
hidden_size = 2

# 构建RNN
cell = nn.RNNCell(input_size=input_size, hidden_size=hidden_size)

# (seq, batch, features)
dataset = torch.randn(seq_len, batch_size, input_size)
hidden = torch.zeros(batch_size, hidden_size)

for idx, input_num in enumerate(dataset, 0):
    print(&#39;=&#39; * 20, idx, &#39;=&#39; * 20)
    print(&#39;Input size:&#39;, input_num.shape)

    hidden = cell(input_num, hidden)

    print(&#39;outputs size:&#39;, hidden.shape)
    print(hidden.data)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 第二种实现：</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  cell = nn.RNN(input_size=inputsize, hidden_size=hidden_size, num_layer=num_layers)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p><code>num_layer</code>表示有多少层。</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615183435636.png" alt="image-20230615183435636"></p>
</li>
<li><p>上述图中<code>inputs</code>就是<code>x1-xN</code>，<code>hidden</code>就是<code>h0</code>的值。</p>
</li>
<li><p>如果采用该种方法实现，输入的长度以及输出的长度：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615190625148.png" alt="image-20230615190625148"></p>
</li>
<li><p>举个栗子：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615190709953.png" alt="image-20230615190709953"></p>
</li>
<li><p><code>numLayers</code>如下：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230615190800051.png" alt="image-20230615190800051"></p>
</li>
<li><p>同一颜色的RNN共享权重。</p>
</li>
<li><p>相应代码：</p>
</li>
<li><pre><code class="python"># 设置参数
batch_size = 1
seq_len = 3
input_size = 4
hidden_size = 2
num_layers = 1

cell = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)

# (seqLen, batchSize, inputSize)
inputs = torch.randn(seq_len, batch_size, input_size)
hidden = torch.zeros(num_layers, batch_size, hidden_size)

out, hidden = cell(inputs, hidden)

print(&#39;Output Size:&#39;, out.shape)
print(&#39;Output:&#39;, out.data)
print(&#39;Hidden Size:&#39;, hidden.shape)
print(&#39;Hidden:&#39;, hidden.data)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- RNN的其它参数</span><br><span class="line"></span><br><span class="line">  - `batch_first`为`True`我们可以知道：要将`seqLen`与`BatchSize`做交换。</span><br><span class="line">  - ![image-20230615191909851](../pictures/pytorch快速入门/image-20230615191909851.png)</span><br><span class="line"></span><br><span class="line">- 一个实栗</span><br><span class="line"></span><br><span class="line">  - 将&quot;Hello&quot;-----&gt;&quot;ohlol&quot;</span><br><span class="line"></span><br><span class="line">  - ![image-20230615192113064](../pictures/pytorch快速入门/image-20230615192113064.png)</span><br><span class="line"></span><br><span class="line">  - 输入必须是向量，所以输入需要做一个转换：</span><br><span class="line"></span><br><span class="line">  - ![image-20230615192211066](../pictures/pytorch快速入门/image-20230615192211066.png)</span><br><span class="line"></span><br><span class="line">  - 再变成索引：</span><br><span class="line"></span><br><span class="line">  - ![image-20230615192244541](../pictures/pytorch快速入门/image-20230615192244541.png)</span><br><span class="line"></span><br><span class="line">  - 再转变：</span><br><span class="line"></span><br><span class="line">  - ![image-20230615192321289](../pictures/pytorch快速入门/image-20230615192321289.png)</span><br><span class="line"></span><br><span class="line">  - 代码如下：</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    import torch</span><br><span class="line">    from torch import nn</span><br><span class="line">    </span><br><span class="line">    # 参数设置</span><br><span class="line">    input_size = 4</span><br><span class="line">    hidden_size = 4</span><br><span class="line">    batch_size = 1</span><br><span class="line">    </span><br><span class="line">    # 准备数据</span><br><span class="line">    idx2char = [&#x27;e&#x27;, &#x27;h&#x27;, &#x27;l&#x27;, &#x27;o&#x27;]</span><br><span class="line">    x_data = [1, 0, 2, 2, 3]</span><br><span class="line">    y_data = [3, 1, 2, 3, 2]</span><br><span class="line">    </span><br><span class="line">    one_hot_lookup = [[1, 0, 0, 0],</span><br><span class="line">                      [0, 1, 0, 0],</span><br><span class="line">                      [0, 0, 1, 0],</span><br><span class="line">                      [0, 0, 0, 1]]</span><br><span class="line">    x_one_hot = [one_hot_lookup[x] for x in x_data]</span><br><span class="line">    </span><br><span class="line">    inputs = torch.Tensor(x_one_hot).reshape(-1, batch_size, input_size)</span><br><span class="line">    labels = torch.LongTensor(y_data).reshape(-1, 1)</span><br><span class="line">    </span><br><span class="line">    # 定义模型</span><br><span class="line">    class Net(nn.Module):</span><br><span class="line">        def __init__(self, input_size, hidden_size, batch_size):</span><br><span class="line">            super(Net, self).__init__()</span><br><span class="line">            self.input_size = input_size</span><br><span class="line">            self.hidden_size = hidden_size</span><br><span class="line">            self.batch_size = batch_size</span><br><span class="line">            self.model = nn.RNNCell(input_size=self.input_size, hidden_size=self.hidden_size)</span><br><span class="line">    </span><br><span class="line">        def forward(self, inputs, hidden):</span><br><span class="line">            hidden = self.model(inputs, hidden)</span><br><span class="line">            return hidden</span><br><span class="line">    </span><br><span class="line">        def init_hidden(self):</span><br><span class="line">            return torch.zeros(self.batch_size, self.hidden_size)</span><br><span class="line">    </span><br><span class="line">    net = Net(input_size=input_size, hidden_size=hidden_size, batch_size=batch_size)</span><br><span class="line">    </span><br><span class="line">    # 优化器和损失函数</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=1e-1)</span><br><span class="line">    </span><br><span class="line">    # 训练</span><br><span class="line">    for epoch in range(15):</span><br><span class="line">        loss_sum = 0</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        hidden = net.init_hidden()</span><br><span class="line">        print(&#x27;Predicted string:&#x27;, end=&#x27;&#x27;)</span><br><span class="line">        for input_num, label in zip(inputs, labels):</span><br><span class="line">            hidden = net(input_num, hidden)</span><br><span class="line">            loss_sum += loss(hidden, label)</span><br><span class="line">            _, idx = hidden.max(dim=1)</span><br><span class="line">            print(idx2char[idx.item()], end=&#x27;&#x27;)</span><br><span class="line">        loss_sum.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        print(&#x27;, Epoch [%d/15] loss=%.4f&#x27; % (epoch + 1, loss_sum.item()))</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>用RNN训练：</p>
</li>
<li><pre><code class="python">import torch
from torch import nn

# 设置设备
device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

# 设置参数
input_size = 4
hidden_size = 4
batch_size = 1
num_layers = 1
seq_len = 5

# 加载数据
idx2char = [&#39;h&#39;, &#39;e&#39;, &#39;l&#39;, &#39;o&#39;]
ont_hot_lookup = [[1, 0, 0, 0],
                  [0, 1, 0, 0],
                  [0, 0, 1, 0],
                  [0, 0, 0, 1]]
x_data = [0, 1, 2, 2, 3]
y_data = [3, 0, 2, 3, 2]
x_one_hot = [ont_hot_lookup[x] for x in x_data]
x_one_hot = torch.tensor(x_one_hot, dtype=torch.float32).reshape(seq_len, batch_size, input_size)
labels = torch.LongTensor(y_data)

# 构建模型
class Net(nn.Module):
    def __init__(self, input_size, hidden_size, batch_size, num_layers):
        super(Net, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.batch_size = batch_size
        self.num_layers = num_layers
        self.Model = nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers)

    def forward(self, input_num):
        hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size)
        hidden = hidden.to(device)
        out, hidden = self.Model(input_num, hidden)
        return out.reshape(-1, self.hidden_size), hidden

net = Net(input_size, hidden_size, batch_size, num_layers)
net = net.to(device)

# 构建损失函数
loss = nn.CrossEntropyLoss()

# 开始训练
epochs, lr = 15, 0.5
optimizer = torch.optim.Adam(net.parameters(), lr=lr)

for epoch in range(epochs):
    features, labels = x_one_hot.to(device), labels.to(device)
    out, hidden = net(features)
    _, idx = out.max(dim=1)
    output = [idx2char[x] for x in idx.tolist()]
    str1 = &#39;&#39;.join(output)
    print(str1)
    loss_num = loss(out, labels)
    optimizer.zero_grad()
    loss_num.backward()
    optimizer.step()
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 独热向量(One-hot)的一点评价</span><br><span class="line"></span><br><span class="line">  - 独热向量的维度太高。</span><br><span class="line">  - 独热向量的维度太稀疏。</span><br><span class="line">  - 独热向量是硬编码的。</span><br><span class="line">  - 可替代的方案有：`EMBEDDING`</span><br><span class="line"></span><br><span class="line">- **EMBEDDING**</span><br><span class="line"></span><br><span class="line">  - 把一个高维的稀疏样本映射到低维稠密的样本</span><br><span class="line"></span><br><span class="line">  - ![image-20230615200526789](../pictures/pytorch快速入门/image-20230615200526789.png)</span><br><span class="line"></span><br><span class="line">  - 也就是数据的降维。</span><br><span class="line"></span><br><span class="line">  - 网络进行变化：</span><br><span class="line"></span><br><span class="line">  - ![image-20230615200705834](../pictures/pytorch快速入门/image-20230615200705834.png)</span><br><span class="line"></span><br><span class="line">  - 对于输入需要是`LongTensor`类型张量。这里输出是`(seqLen, 4)`数据。</span><br><span class="line"></span><br><span class="line">  - 模型改进：</span><br><span class="line"></span><br><span class="line">  - ![image-20230615201003076](../pictures/pytorch快速入门/image-20230615201003076.png)</span><br><span class="line"></span><br><span class="line">  - ![image-20230615201052585](../pictures/pytorch快速入门/image-20230615201052585.png)</span><br><span class="line"></span><br><span class="line">  - ![image-20230615201107118](../pictures/pytorch快速入门/image-20230615201107118.png)</span><br><span class="line"></span><br><span class="line">  - ![image-20230615201118263](../pictures/pytorch快速入门/image-20230615201118263.png)</span><br><span class="line"></span><br><span class="line">  - ![image-20230615201129290](../pictures/pytorch快速入门/image-20230615201129290.png)</span><br><span class="line"></span><br><span class="line">  - 代码如下：</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    import torch</span><br><span class="line">    from torch import nn</span><br><span class="line">    </span><br><span class="line">    # 设置设备</span><br><span class="line">    device = torch.device(&#x27;cuda:0&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line">    </span><br><span class="line">    # 设置参数</span><br><span class="line">    input_size = 4</span><br><span class="line">    hidden_size = 8</span><br><span class="line">    batch_size = 1</span><br><span class="line">    num_layers = 2</span><br><span class="line">    seq_len = 5</span><br><span class="line">    embedding_size = 10</span><br><span class="line">    num_class = 4</span><br><span class="line">    </span><br><span class="line">    # 加载数据</span><br><span class="line">    idx2char = [&#x27;h&#x27;, &#x27;e&#x27;, &#x27;l&#x27;, &#x27;o&#x27;]</span><br><span class="line">    x_data = [[0, 1, 2, 2, 3]]</span><br><span class="line">    y_data = [3, 0, 2, 3, 2]</span><br><span class="line">    labels = torch.LongTensor(y_data)</span><br><span class="line">    features = torch.LongTensor(x_data)</span><br><span class="line">    </span><br><span class="line">    # 构建模型</span><br><span class="line">    class Net(nn.Module):</span><br><span class="line">        def __init__(self, input_size, hidden_size, batch_size, num_layers, num_class, embedding_size):</span><br><span class="line">            super(Net, self).__init__()</span><br><span class="line">            self.input_size = input_size</span><br><span class="line">            self.hidden_size = hidden_size</span><br><span class="line">            self.batch_size = batch_size</span><br><span class="line">            self.num_layers = num_layers</span><br><span class="line">            self.num_class = num_class</span><br><span class="line">            self.embedding_size = embedding_size</span><br><span class="line">            self.emb = nn.Embedding(self.input_size, self.embedding_size)</span><br><span class="line">            self.Model = nn.RNN(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers,</span><br><span class="line">                                batch_first=True)</span><br><span class="line">            self.fc = nn.Linear(self.hidden_size, self.num_class)</span><br><span class="line">    </span><br><span class="line">        def forward(self, input_num):</span><br><span class="line">            hidden = torch.zeros(self.num_layers, input_num.shape[0], self.hidden_size)</span><br><span class="line">            hidden = hidden.to(device)</span><br><span class="line">            input_num = self.emb(input_num)</span><br><span class="line">            out, hidden = self.Model(input_num, hidden)</span><br><span class="line">            out = self.fc(out)</span><br><span class="line">            hidden = self.fc(hidden)</span><br><span class="line">            return out.reshape(-1, self.num_class), hidden</span><br><span class="line">    </span><br><span class="line">    net = Net(input_size, hidden_size, batch_size, num_layers, num_class, embedding_size)</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    </span><br><span class="line">    # 构建损失函数</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    </span><br><span class="line">    # 开始训练</span><br><span class="line">    epochs, lr = 15, 0.05</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    </span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        features, labels = features.to(device), labels.to(device)</span><br><span class="line">        out, hidden = net(features)</span><br><span class="line">        _, idx = out.max(dim=1)</span><br><span class="line">        output = [idx2char[x] for x in idx.tolist()]</span><br><span class="line">        str1 = &#x27;&#x27;.join(output)</span><br><span class="line">        print(str1)</span><br><span class="line">        loss_num = loss(out, labels)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss_num.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>扩展：学习<code>LSTM</code>模型。</p>
</li>
</ul>
</li>
<li><p>循环卷积的一点理解：</p>
<ul>
<li><code>seq_len</code>表示句子的长度</li>
<li><code>batch_size</code>表示有几个句子</li>
<li><code>input_size</code>表示每个句子的每个字用多长的向量表示</li>
<li><code>output_size</code>表示经过网络输出，一个字变为用多少个字表示</li>
<li>对于输入只输入<code>(batch_size, input_size)</code>其实就是通过for循环将句子的第一个字进行传入模型进行训练。对于RNN没办法自动传，需要自己手动给长度。</li>
</ul>
</li>
</ul>
<h2 id="13-循环神经网络-高级篇"><a href="#13-循环神经网络-高级篇" class="headerlink" title="13 循环神经网络(高级篇)"></a>13 循环神经网络(高级篇)</h2><ul>
<li><p>本章实现一个名字分类的挑战。</p>
<ul>
<li>给出一组数据，也就是名字，我们要通过训练模型实现来判断名字是那个国家。</li>
</ul>
</li>
<li><p>我们的模型如下：</p>
<ul>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230616141121024.png" alt="image-20230616141121024"></li>
<li>也就是下面的简化图：</li>
<li><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230616141325644.png" alt="image-20230616141325644"></li>
</ul>
</li>
<li><p>数据的处理</p>
<ul>
<li><p>我们拿到的数据是字符串，要将字符串转化为列表。因为都是英文字符，所以我们做<code>ASCII</code>表即可。</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230616142344487.png" alt="image-20230616142344487"></p>
</li>
<li><p>由于序列的长短不一，我们需要进行补零操作使得序列的长度相等。</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230616142508026.png" alt="image-20230616142508026"></p>
</li>
<li></li>
<li><p>处理好名字再处理国家</p>
</li>
<li><p>我们要把国家转成分类索引：</p>
</li>
<li><p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230616142712545.png" alt="image-20230616142712545"></p>
</li>
<li><p>整体代码：</p>
</li>
<li><pre><code class="python">import csv
import gzip
import torch
from torch import nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pack_padded_sequence

# 设备准备
device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)

# 参数准备
BATCH_SIZE = 256
HIDDEN_SIZE = 100
N_LAYER = 2
N_EPOCH = 100
N_CHARS = 128 # 128个字典长度，独热编码

# 数据准备
class NameDataset(Dataset):
    def __init__(self, is_train_set=True):
        filename = &#39;names_train.csv.gz&#39; if is_train_set else &#39;names_test.csv.gz&#39;
        with gzip.open(filename, &#39;rt&#39;) as f:
            reader = csv.reader(f)
            rows = list(reader)
        self.names = [row[0] for row in rows]
        self.len = len(self.names)
        self.countries = [row[1] for row in rows]
        self.country_list = list(sorted(set(self.countries)))
        self.country_dict = &#123;country: index for index, country in enumerate(self.country_list, 0)&#125;
        self.country_num = len(self.country_list)

    def __getitem__(self, item):
        return self.names[item], self.country_dict[self.countries[item]]

    def __len__(self):
        return self.len

    def getCountryDict(self):
        country_dict = dict()
        for idx, country_name in enumerate(self.country_list, 0):
            country_dict[country_name] = idx
            return country_dict

    def idx2country(self, index):
        return self.country_list[index]

    def getCountriesNum(self):
        return self.country_num

train_set = NameDataset(is_train_set=True)
train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)
test_set = NameDataset(is_train_set=False)
test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)

# 输出尺度大小
N_COUNTRY = train_set.getCountriesNum()

# 构建模型
class RNNClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):
        super(RNNClassifier, self).__init__()
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.n_directions = 2 if bidirectional else 1
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, bidirectional=bidirectional)
        self.fc = nn.Linear(hidden_size * self.n_directions, output_size)

    def _init_hidden(self, batch_size):
        hidden = torch.zeros(self.n_layers * self.n_directions, batch_size, self.hidden_size)
        return hidden.to(device)

    def forward(self, input_num, seq_lengths):
        input_num = input_num.t()
        batch_size = input_num.size(1)

        hidden = self._init_hidden(batch_size)
        embedding = self.embedding(input_num)

        gru_input = pack_padded_sequence(embedding, seq_lengths)
        output, hidden = self.gru(gru_input, hidden)
        if self.n_directions == 2:
            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1)
        else:
            hidden_cat = hidden[-1]
        fc_output = self.fc(hidden_cat)
        return fc_output
classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)
classifier = classifier.to(device)

# 数据转Tensor
def name2list(name):
    arr = [ord(c) for c in name]
    return arr, len(arr)

def make_tensors(names, countries):
    sequences_and_lengths = [name2list(name) for name in names]
    name_sequences = [sl[0] for sl in sequences_and_lengths]
    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths])
    countries = countries.long()

    # make tensor of name, BatchSize x seqLen
    seq_tensor = torch.zeros(len(name_sequences), seq_lengths.max()).long()
    for idx, (seq, seq_len) in enumerate(zip(name_sequences, seq_lengths), 0):
        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)

    # sort by length to use pack_padded_sequence
    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)
    seq_tensor = seq_tensor[perm_idx]
    countries = countries[perm_idx]

    return seq_tensor.to(device), seq_lengths.to(device), countries.to(device)

# 一些优化器
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)

def train_model():
    total_loss = 0
    for i, (names, countries) in enumerate(train_loader, 1):
        inputs, seq_lengths, target = make_tensors(names, countries)
        output = classifier(inputs, seq_lengths.to(&#39;cpu&#39;))
        loss = criterion(output, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        if i % 10 == 0:
            print(f&#39;[&#123;i * len(inputs)&#125;/&#123;len(train_set)&#125;]&#39;, end=&#39;&#39;)
            print(f&#39;loss=&#123;total_loss / (i * len(inputs))&#125;&#39;)
    return total_loss

def test_model():
    correct = 0
    total = len(test_set)
    print(&#39;evaluating trained model ...&#39;)
    with torch.no_grad():
        for i, (names, countries) in enumerate(test_loader, 1):
            inputs, seq_lengths, target = make_tensors(names, countries)
            output = classifier(inputs, seq_lengths.to(&#39;cpu&#39;))
            pred = output.max(dim=1, keepdim=True)[1]
            correct += pred.eq(target.view_as(pred)).sum().item()

        percent = &#39;%.2f&#39; % (100 * correct / total)
        print(f&#39;Test set: Accuracy &#123;correct&#125;/&#123;total&#125;  &#123;percent&#125;%&#39;)
    return correct / total

# 开始训练
acc_list = []
for epoch in range(1, N_EPOCH + 1):
    train_model()
    acc = test_model()
    acc_list.append(acc)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"># Pytorch入门项目</span><br><span class="line"></span><br><span class="line">## 1 基础</span><br><span class="line"></span><br><span class="line">### PyTorch基础</span><br><span class="line"></span><br><span class="line">## 4 TensorBoard</span><br><span class="line"></span><br><span class="line">### 官方文档</span><br><span class="line"></span><br><span class="line">`TensorBoard`是用于可视化的工具，它和动态绘图类似，但是比动态绘图更为强大。接下来的教程参考Pytorch官方教程来讲解以及一个实际栗子来讲解TensorBoard在Pytorch中的应用再加上git的代码来讲解。</span><br><span class="line"></span><br><span class="line">我们再`TensorBoard`中可以观察训练的标量结果、图像、直方图、图像和嵌入式可视化等。</span><br><span class="line"></span><br><span class="line">在`TensorBoard`中，我们主要用的功能是`SummaryWriter`，它可以让我们进入可视化界面，举个例子：</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">import torchvision</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line">from torchvision import datasets</span><br><span class="line">from torchvision import transforms</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"></span><br><span class="line">device = torch.device(&#x27;cuda:0&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line"></span><br><span class="line"># writer将把输出进行写入</span><br><span class="line"># 加载数据</span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(),</span><br><span class="line">                                transforms.Normalize((0.5, ), (0.5, ))])</span><br><span class="line">train_set = datasets.MNIST(root=&#x27;./part000tensorboard/data&#x27;, train=True,</span><br><span class="line">                           download=True, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_set, batch_size=64, shuffle=True)</span><br><span class="line"></span><br><span class="line">model = torchvision.models.resnet50(False)</span><br><span class="line">model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)</span><br><span class="line">images, labels = next(iter(train_loader))</span><br><span class="line"></span><br><span class="line">grid = torchvision.utils.make_grid(images)</span><br><span class="line">writer.add_image(&#x27;images&#x27;, grid, 0)</span><br><span class="line">writer.add_graph(model, images)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
<p>我们继续在终端输入命令：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=runs</span><br></pre></td></tr></table></figure>

<p>即可执行。得到的效果图如下：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617125227919-1686977548270-1.png" alt="image-20230617125227919"></p>
<p>我们在一次实验中可以记录大量的信息，为了防止信息的混乱，输出的结果更好，我们应该通过分层命名的方式对图进行分组。如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n_iter <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Loss/train&#x27;</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Loss/test&#x27;</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Accuracy/train&#x27;</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Accuracy/test&#x27;</span>, np.random.random(), n_iter)</span><br></pre></td></tr></table></figure>

<p>得到的结果图如下：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617125157797.png" alt="image-20230617125157797"></p>
<p>一般我们构造会分目录来构造，多人使用同一个服务器可以方便查看自己的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># writer将把输出进行写入</span></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./runs/part000/example_1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(),</span><br><span class="line">                                transforms.Normalize((<span class="number">0.5</span>, ), (<span class="number">0.5</span>, ))])</span><br><span class="line">train_set = datasets.MNIST(root=<span class="string">&#x27;./part000tensorboard/data&#x27;</span>, train=<span class="literal">True</span>,</span><br><span class="line">                           download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">train_loader = DataLoader(train_set, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">model = torchvision.models.resnet50(<span class="literal">False</span>)</span><br><span class="line">model.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">images, labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line"></span><br><span class="line">grid = torchvision.utils.make_grid(images)</span><br><span class="line">writer.add_image(<span class="string">&#x27;images&#x27;</span>, grid, <span class="number">0</span>)</span><br><span class="line">writer.add_graph(model, images)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>此时我们输入命令：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=runs/part000</span><br></pre></td></tr></table></figure>

<p>我们就可以看到里面所有的<code>examples</code>。</p>
<p>我们上文用的<code>add_scalar</code>可以添加一个图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./runs/part000/example_2&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n_iter <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Loss/train&#x27;</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Loss/test&#x27;</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Accuracy/train&#x27;</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;Accuracy/test&#x27;</span>, np.random.random(), n_iter)</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617130601076.png" alt="image-20230617130601076"></p>
<p>如果我们想一张图画多个数据，我们应该使用<code>add_scalars</code>来实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./runs/part000/example_4&#x27;</span>)</span><br><span class="line">r = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalars(<span class="string">&#x27;多三角函数&#x27;</span>, &#123;<span class="string">&#x27;xsinx&#x27;</span>: np.sin(i / <span class="number">5</span>),</span><br><span class="line">                                      <span class="string">&#x27;xcosx&#x27;</span>: np.cos(i / <span class="number">5</span>),</span><br><span class="line">                                      <span class="string">&#x27;tanx&#x27;</span>: np.tan(i / <span class="number">5</span>)&#125;, i)</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617131241836.png" alt="image-20230617131241836"></p>
<p>在这里我们也可以增加柱状图，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./runs/part000/example_5&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    x = np.random.randn(<span class="number">1000</span>)</span><br><span class="line">    writer.add_histogram(<span class="string">&#x27;distribution conters&#x27;</span>, i + <span class="number">1</span>, i)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>效果图如下：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617131806985.png" alt="image-20230617131806985"></p>
<p>当然，我们也可以添加图片的显示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./runs/part000/example_6&#x27;</span>)</span><br><span class="line"><span class="comment"># 显示信息</span></span><br><span class="line">img = np.zeros((<span class="number">3</span>, <span class="number">100</span>, <span class="number">100</span>))</span><br><span class="line">img[<span class="number">0</span>] = np.arange(<span class="number">0</span>, <span class="number">10000</span>).reshape(<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line">img[<span class="number">1</span>] = <span class="number">1</span> - np.arange(<span class="number">0</span>, <span class="number">10000</span>).reshape(<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line">img_HWC = np.zeros((<span class="number">100</span>, <span class="number">100</span>, <span class="number">3</span>))</span><br><span class="line">img_HWC[:, :, <span class="number">0</span>] = np.arange(<span class="number">0</span>, <span class="number">10000</span>).reshape(<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line">img_HWC[:, :, <span class="number">1</span>] = <span class="number">1</span> - np.arange(<span class="number">0</span>, <span class="number">10000</span>).reshape(<span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line"><span class="comment"># 进行显示</span></span><br><span class="line">writer.add_image(<span class="string">&#x27;images&#x27;</span>, img, <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 如果图片不是默认维度，我们需要自己指定维度</span></span><br><span class="line">writer.add_image(<span class="string">&#x27;images_HWC&#x27;</span>, img_HWC, <span class="number">0</span>, dataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>效果如下：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617132858715.png" alt="image-20230617132858715"></p>
<p>如果我们需要一次性显示多张图，我们可以使用如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img_batch = np.zeros((<span class="number">16</span>, <span class="number">3</span>, <span class="number">100</span>, <span class="number">100</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">16</span>):</span><br><span class="line">    img_batch[i, <span class="number">0</span>] = np.arange(<span class="number">0</span>, <span class="number">10000</span>).reshape(<span class="number">100</span>, <span class="number">100</span>) / <span class="number">10000</span> / <span class="number">16</span> * i</span><br><span class="line">    img_batch[i, <span class="number">1</span>] = (<span class="number">1</span> - np.arange(<span class="number">0</span>, <span class="number">10000</span>).reshape(<span class="number">100</span>, <span class="number">100</span>) / <span class="number">10000</span>) / <span class="number">16</span> * i</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./runs/part000/example_7&#x27;</span>)</span><br><span class="line">writer.add_images(<span class="string">&#x27;my_image_batch&#x27;</span>, img_batch, <span class="number">0</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>效果图如下：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617133222942.png" alt="image-20230617133222942"></p>
<p>我们事先要将图片信息全部放到信息变量中然后一次性传输再显示。</p>
<p>剩下的具体见文档：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensorboard.html">torch.utils.tensorboard — PyTorch 2.0 documentation</a></p>
<p>剩下的主要的功能如下：</p>
<ul>
<li>将<code>matplotlib</code>图片显示</li>
<li>添加音频</li>
<li>添加文本显示</li>
<li>添加图标</li>
<li>添加三维图</li>
</ul>
<h3 id="官方文档的一个栗子"><a href="#官方文档的一个栗子" class="headerlink" title="官方文档的一个栗子"></a>官方文档的一个栗子</h3><p>文档的出处：<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html">Visualizing Models, Data, and Training with TensorBoard — PyTorch Tutorials 2.0.1+cu117 documentation</a></p>
<p>通过该栗子，我们应该会基础的<code>tensorboard</code>的操作。</p>
<p>首先构建1数据和模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据以及初始准备</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./runs/part000/example_8&#x27;</span>)</span><br><span class="line"><span class="comment"># transforms</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(),</span><br><span class="line">                                 transforms.Normalize((<span class="number">0.5</span>, ), (<span class="number">0.5</span>, ))])</span><br><span class="line">train_set = torchvision.datasets.FashionMNIST(<span class="string">&#x27;./part000tensorboard/data&#x27;</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                              transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_set = torchvision.datasets.FashionMNIST(<span class="string">&#x27;./part000tensorboard/data&#x27;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                              transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># dataloaders</span></span><br><span class="line">train_losder = DataLoader(train_set, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_set, batch_size=<span class="number">4</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># classes</span></span><br><span class="line">classes = (<span class="string">&#x27;T-shirt/top&#x27;</span>, <span class="string">&#x27;Trouser&#x27;</span>, <span class="string">&#x27;Pullover&#x27;</span>, <span class="string">&#x27;Dress&#x27;</span>, <span class="string">&#x27;Coat&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Sandal&#x27;</span>, <span class="string">&#x27;Shirt&#x27;</span>, <span class="string">&#x27;Sneaker&#x27;</span>, <span class="string">&#x27;Bag&#x27;</span>, <span class="string">&#x27;Ankle Boot&#x27;</span>)</span><br><span class="line"><span class="comment"># 帮助显示图片</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matplotlib_imshow</span>(<span class="params">img, one_channel=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> one_channel:</span><br><span class="line">        img = img.mean(dim=<span class="number">0</span>)</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    <span class="keyword">if</span> one_channel:</span><br><span class="line">        plt.imshow(npimg, cmap=<span class="string">&quot;Greys&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>

<p>往<code>tensorboard</code>写数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写入随机的图片</span></span><br><span class="line">images, labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line"><span class="comment"># 图像创造栅格，将多张图片组成一张</span></span><br><span class="line">img_grid = torchvision.utils.make_grid(images)</span><br><span class="line"><span class="comment"># 显示图片</span></span><br><span class="line">matplotlib_imshow(img_grid, one_channel=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 写进tensorboard</span></span><br><span class="line">writer.add_image(<span class="string">&#x27;four_fashion_mnist_images&#x27;</span>, img_grid)</span><br></pre></td></tr></table></figure>

<p>现在输入下面代码进行运行<code>tensorboard</code>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=runs/part000</span><br></pre></td></tr></table></figure>

<p>得到的结果如下：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617191731170.png" alt="image-20230617191731170"></p>
<p>现在我们尝试显示模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">writer.add_graph(net, images)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>点开<code>tensorboard</code>如下：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617191930881.png" alt="image-20230617191930881"></p>
<p>双击可以展开：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617192023977.png" alt="image-20230617192023977"></p>
<p><code>TensorBoard</code>有一个非常方便的功能，可以将高维数据(如图像数据)在低维空间中可视化。具体介绍如下：</p>
<p>我们可以通过<code>add_embedding</code>的办法将高维数据进行低维的可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">select_n_random</span>(<span class="params">data, labels, n=<span class="number">100</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    选择n个随机数据</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(data) == <span class="built_in">len</span>(labels)</span><br><span class="line">    <span class="comment"># 选择一个随机的行，对应列</span></span><br><span class="line">    perm = torch.randperm(<span class="built_in">len</span>(data))</span><br><span class="line">    <span class="keyword">return</span> data[perm][:n], labels[perm][:n]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择随机照片和它们的索引</span></span><br><span class="line">images, labels = select_n_random(train_set.data, train_set.targets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到土拍你的标签</span></span><br><span class="line">class_labels = [classes[lab] <span class="keyword">for</span> lab <span class="keyword">in</span> labels]</span><br><span class="line"></span><br><span class="line"><span class="comment"># log embeddings</span></span><br><span class="line">features = images.view(-<span class="number">1</span>, <span class="number">28</span> * <span class="number">28</span>)</span><br><span class="line">writer.add_embedding(features, metadata=class_labels, label_img=images.unsqueeze(<span class="number">1</span>))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>现在在TensorBoard的 “Projector “选项卡中，你可以看到这100张图片–每张都是784维的–投射到三维空间。此外，这是互动的：你可以点击和拖动来旋转三维投影。最后，有几个提示，使可视化更容易看到：在左上方选择 “颜色：标签”，以及启用 “夜间模式”，这将使图像更容易看到，因为它们的背景是白色的：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617193901207.png" alt="image-20230617193901207"></p>
<p>基本使用结束，我们现在试着让模型评估更加清楚：</p>
<p>现在我们将损失打印到<code>tensorboard</code>进行查看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># helper functions</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">images_to_probs</span>(<span class="params">net, images</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Generates predictions and corresponding probabilities from a trained</span></span><br><span class="line"><span class="string">    network and a list of images</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    output = net(images)</span><br><span class="line">    <span class="comment"># convert output probabilities to predicted class</span></span><br><span class="line">    _, preds_tensor = torch.<span class="built_in">max</span>(output, <span class="number">1</span>)</span><br><span class="line">    preds = np.squeeze(preds_tensor.numpy())</span><br><span class="line">    <span class="keyword">return</span> preds, [F.softmax(el, dim=<span class="number">0</span>)[i].item() <span class="keyword">for</span> i, el <span class="keyword">in</span> <span class="built_in">zip</span>(preds, output)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_classes_preds</span>(<span class="params">net, images, labels</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Generates matplotlib Figure using a trained network, along with images</span></span><br><span class="line"><span class="string">    and labels from a batch, that shows the network&#x27;s top prediction along</span></span><br><span class="line"><span class="string">    with its probability, alongside the actual label, coloring this</span></span><br><span class="line"><span class="string">    information based on whether the prediction was correct or not.</span></span><br><span class="line"><span class="string">    Uses the &quot;images_to_probs&quot; function.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    preds, probs = images_to_probs(net, images)</span><br><span class="line">    <span class="comment"># plot the images in the batch, along with predicted and true labels</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">12</span>, <span class="number">48</span>))</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> np.arange(<span class="number">4</span>):</span><br><span class="line">        ax = fig.add_subplot(<span class="number">1</span>, <span class="number">4</span>, idx+<span class="number">1</span>, xticks=[], yticks=[])</span><br><span class="line">        matplotlib_imshow(images[idx], one_channel=<span class="literal">True</span>)</span><br><span class="line">        ax.set_title(<span class="string">&quot;&#123;0&#125;, &#123;1:.1f&#125;%\n(label: &#123;2&#125;)&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">            classes[preds[idx]],</span><br><span class="line">            probs[idx] * <span class="number">100.0</span>,</span><br><span class="line">            classes[labels[idx]]),</span><br><span class="line">                    color=(<span class="string">&quot;green&quot;</span> <span class="keyword">if</span> preds[idx]==labels[idx].item() <span class="keyword">else</span> <span class="string">&quot;red&quot;</span>))</span><br><span class="line">    <span class="keyword">return</span> fig</span><br></pre></td></tr></table></figure>

<p>开始训练模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">running_loss = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">999</span>:    <span class="comment"># every 1000 mini-batches...</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># ...log the running loss</span></span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;training loss&#x27;</span>,</span><br><span class="line">                            running_loss / <span class="number">1000</span>,</span><br><span class="line">                            epoch * <span class="built_in">len</span>(train_loader) + i)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ...log a Matplotlib Figure showing the model&#x27;s predictions on a</span></span><br><span class="line">            <span class="comment"># random mini-batch</span></span><br><span class="line">            writer.add_figure(<span class="string">&#x27;predictions vs. actuals&#x27;</span>,</span><br><span class="line">                            plot_classes_preds(net, inputs, labels),</span><br><span class="line">                            global_step=epoch * <span class="built_in">len</span>(train_loader) + i)</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>此时得到的结果：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617194609375.png" alt="image-20230617194609375"></p>
<p>另外我们可以查看预测结果：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617195027586.png" alt="image-20230617195027586"></p>
<p>评估模型训练的好坏</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. gets the probability predictions in a test_size x num_classes Tensor</span></span><br><span class="line"><span class="comment"># 2. gets the preds in a test_size Tensor</span></span><br><span class="line"><span class="comment"># takes ~10 seconds to run</span></span><br><span class="line">class_probs = []</span><br><span class="line">class_label = []</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        output = net(images)</span><br><span class="line">        class_probs_batch = [F.softmax(el, dim=<span class="number">0</span>) <span class="keyword">for</span> el <span class="keyword">in</span> output]</span><br><span class="line"></span><br><span class="line">        class_probs.append(class_probs_batch)</span><br><span class="line">        class_label.append(labels)</span><br><span class="line"></span><br><span class="line">test_probs = torch.cat([torch.stack(batch) <span class="keyword">for</span> batch <span class="keyword">in</span> class_probs])</span><br><span class="line">test_label = torch.cat(class_label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># helper function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_pr_curve_tensorboard</span>(<span class="params">class_index, test_probs, test_label, global_step=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Takes in a &quot;class_index&quot; from 0 to 9 and plots the corresponding</span></span><br><span class="line"><span class="string">    precision-recall curve</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    tensorboard_truth = test_label == class_index</span><br><span class="line">    tensorboard_probs = test_probs[:, class_index]</span><br><span class="line"></span><br><span class="line">    writer.add_pr_curve(classes[class_index],</span><br><span class="line">                        tensorboard_truth,</span><br><span class="line">                        tensorboard_probs,</span><br><span class="line">                        global_step=global_step)</span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot all the pr curves</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(classes)):</span><br><span class="line">    add_pr_curve_tensorboard(i, test_probs, test_label)</span><br></pre></td></tr></table></figure>

<p>我们可以在<code>PR Curves</code>查看：</p>
<p><img src="/../pictures/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image-20230617195707730.png" alt="image-20230617195707730"></p>
<h3 id="git项目"><a href="#git项目" class="headerlink" title="git项目"></a>git项目</h3><p>鉴于项目用的<code>tensorflow</code>代码写，暂时不看。</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">John Doe</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://example.com/2023/09/02/pytorch%E5%AD%A6%E4%B9%A0-pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/">http://example.com/2023/09/02/pytorch%E5%AD%A6%E4%B9%A0-pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">John Doe</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/pytorch%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">pytorch学习</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2023/09/02/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/16.jpg" class="responsive-img" alt="动手学习深度学习-2-预备知识">
                        
                        <span class="card-title">动手学习深度学习-2-预备知识</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2023-09-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            John Doe
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">动手学习深度学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2023/09/02/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-Review-on-Deep-Neural-Networks-Applied-to-Low-Frequency-NILM/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/14.jpg" class="responsive-img" alt="文献阅读-Review on Deep Neural Networks Applied to Low-Frequency NILM">
                        
                        <span class="card-title">文献阅读-Review on Deep Neural Networks Applied to Low-Frequency NILM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2023-09-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            John Doe
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/NILM%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">
                        <span class="chip bg-color">NILM相关文献阅读</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2023</span>
            
            <a href="/about" target="_blank">John Doe</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Bktou" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:384096251@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=384096251" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 384096251" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/sakura.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
